{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import PIL\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Image Feature and Slide Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_tf(tf_path):\n",
    "    feature = {'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "               'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "               'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "               'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "               'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "               'image_name': tf.io.FixedLenFeature([], tf.string),\n",
    "               'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "               'image_feature': tf.io.FixedLenFeature([], tf.string)}\n",
    "\n",
    "    tfrecord_dataset = tf.data.TFRecordDataset(tf_path)\n",
    "\n",
    "    def _parse_image_function(key):\n",
    "        return tf.io.parse_single_example(key, feature)\n",
    "\n",
    "    CLAM_dataset = tfrecord_dataset.map(_parse_image_function)\n",
    "\n",
    "    image_features = list()\n",
    "\n",
    "    for tfrecord_value in CLAM_dataset:\n",
    "        img_feature = tf.io.parse_tensor(tfrecord_value['image_feature'], 'float32')\n",
    "        slide_labels = tfrecord_value['label']\n",
    "        slide_label = int(slide_labels)\n",
    "        slide_true = tf.one_hot([slide_label],2)\n",
    "        image_features.append(img_feature)\n",
    "\n",
    "    return image_features, slide_label, slide_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CLAM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import None-Gated Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None-Gated Attention Network Class - assign the same weights of each attention head/layer\n",
    "class NG_Att_Net(tf.keras.Model):\n",
    "    def __init__(self, dim_features=1024, dim_compress_features=512, n_hidden_units=256, n_classes=2,\n",
    "                 dropout=False, dropout_rate=.25):\n",
    "        super(NG_Att_Net, self).__init__()\n",
    "        self.dim_features = dim_features\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.compression_model = tf.keras.models.Sequential()\n",
    "        self.model = tf.keras.models.Sequential()\n",
    "\n",
    "        self.fc_compress_layer = tf.keras.layers.Dense(units=dim_compress_features, activation='relu',\n",
    "                                                       input_shape=(dim_features,), kernel_initializer='glorot_normal',\n",
    "                                                       bias_initializer='zeros', name='Fully_Connected_Layer')\n",
    "\n",
    "        self.compression_model.add(self.fc_compress_layer)\n",
    "        self.model.add(self.fc_compress_layer)\n",
    "\n",
    "        self.att_layer1 = tf.keras.layers.Dense(units=n_hidden_units, activation='tanh',\n",
    "                                                input_shape=(dim_compress_features,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer1')\n",
    "\n",
    "        self.att_layer2 = tf.keras.layers.Dense(units=n_classes, activation='linear', input_shape=(n_hidden_units,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer2')\n",
    "\n",
    "        self.model.add(self.att_layer1)\n",
    "\n",
    "        if dropout:\n",
    "            self.model.add(tf.keras.layers.Dropout(dropout_rate, name='Dropout_Layer'))\n",
    "\n",
    "        self.model.add(self.att_layer2)\n",
    "\n",
    "    def att_compress_model_no_gate(self):\n",
    "        return self.compression_model\n",
    "\n",
    "    def att_model_no_gate(self):\n",
    "        return self.model\n",
    "\n",
    "    def call(self, x):\n",
    "        h = list()\n",
    "        A = list()\n",
    "        \n",
    "        for i in x:\n",
    "            c_imf = self.compression_model(i)\n",
    "            h.append(c_imf)\n",
    "        \n",
    "        for i in x:\n",
    "            a = self.model(i)\n",
    "            A.append(a)\n",
    "        return h, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Gated Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gated Attention Network Class - scaling the weights of each attention head/layer -> weights of each attention layer\n",
    "# would be different\n",
    "class G_Att_Net(tf.keras.Model):\n",
    "    def __init__(self, dim_features=1024, dim_compress_features=512, n_hidden_units=256, n_classes=2,\n",
    "                 dropout=False, dropout_rate=.25):\n",
    "        super(G_Att_Net, self).__init__()\n",
    "        self.dim_features = dim_features\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.compression_model = tf.keras.models.Sequential()\n",
    "        self.model1 = tf.keras.models.Sequential()\n",
    "        self.model2 = tf.keras.models.Sequential()\n",
    "        self.model = tf.keras.models.Sequential()\n",
    "\n",
    "        # GlorotNormal <==> Xavier for weights initialization\n",
    "        self.fc_compress_layer = tf.keras.layers.Dense(units=dim_compress_features, activation='relu',\n",
    "                                                       input_shape=(dim_features,), kernel_initializer='glorot_normal',\n",
    "                                                       bias_initializer='zeros', name='Fully_Connected_Layer')\n",
    "\n",
    "        self.compression_model.add(self.fc_compress_layer)\n",
    "        self.model1.add(self.fc_compress_layer)\n",
    "        self.model2.add(self.fc_compress_layer)\n",
    "\n",
    "        self.att_layer1 = tf.keras.layers.Dense(units=n_hidden_units, activation='tanh', input_shape=(dim_features,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer1')\n",
    "\n",
    "        self.att_layer2 = tf.keras.layers.Dense(units=n_hidden_units, activation='sigmoid', input_shape=(dim_features,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer2')\n",
    "\n",
    "        self.att_layer3 = tf.keras.layers.Dense(units=n_classes, activation='linear', input_shape=(n_hidden_units,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer3')\n",
    "\n",
    "        self.model1.add(self.att_layer1)\n",
    "        self.model2.add(self.att_layer2)\n",
    "\n",
    "        if dropout:\n",
    "            self.model1.add(tf.keras.layers.Dropout(dropout_rate, name='Dropout_Layer'))\n",
    "            self.model2.add(tf.keras.layers.Dropout(dropout_rate, name='Dropout_Layer'))\n",
    "\n",
    "        self.model.add(self.att_layer3)\n",
    "\n",
    "    def att_compress_model_gate(self):\n",
    "        return self.compression_model\n",
    "\n",
    "    def att_model_gate(self):\n",
    "        gated_att_net_list = [self.model1, self.model2, self.model]\n",
    "        return gated_att_net_list\n",
    "\n",
    "    def call(self, x):\n",
    "        h = list()\n",
    "        A = list()\n",
    "        \n",
    "        for i in x:\n",
    "            c_imf = self.compression_model(i)\n",
    "            h.append(c_imf)\n",
    "            \n",
    "        for i in x:\n",
    "            layer1_output = self.model1(i)  # output from the first dense layer\n",
    "            layer2_output = self.model2(i)  # output from the second dense layer\n",
    "            a = tf.math.multiply(layer1_output, layer2_output)  # cross product of the outputs from 1st and 2nd layer\n",
    "            a = self.model(a)  # pass the output of the product of the outputs from 1st 2 layers to the last layer\n",
    "            A.append(a)\n",
    "\n",
    "        return h, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Instance Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_Ins(tf.keras.Model):\n",
    "    def __init__(self, dim_compress_features=512, n_class=2, n_ins=8, mut_ex=False):\n",
    "        super(M_Ins, self).__init__()\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_class = n_class\n",
    "        self.n_ins = n_ins\n",
    "        self.mut_ex = mut_ex\n",
    "        \n",
    "        self.m_ins_models = list()\n",
    "        self.m_ins_model = tf.keras.models.Sequential()\n",
    "        self.m_ins_layer = tf.keras.layers.Dense(\n",
    "        units=self.n_class, activation='linear', input_shape=(self.dim_compress_features,), name='Instance_Classifier_Layer'\n",
    "        )\n",
    "        self.m_ins_model.add(self.m_ins_layer)\n",
    "        \n",
    "        for i in range(self.n_class):\n",
    "            self.m_ins_models.append(self.m_ins_model)\n",
    "    \n",
    "    def m_ins_classifier(self):\n",
    "        return self.m_ins_models\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_pos_labels(n_pos_sample):\n",
    "        return tf.fill(dims=[n_pos_sample, ], value=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_neg_labels(n_neg_sample):\n",
    "        return tf.fill(dims=[n_neg_sample, ], value=0)\n",
    "    \n",
    "    def in_call(self, ins_classifier, h, A_I):\n",
    "        pos_label = self.generate_pos_labels(self.n_ins)\n",
    "        neg_label = self.generate_neg_labels(self.n_ins)\n",
    "        ins_label_in = tf.concat(values=[pos_label, neg_label], axis=0)\n",
    "        A_I = tf.reshape(tf.convert_to_tensor(A_I), (1, len(A_I))) \n",
    "        \n",
    "        top_pos_ids = tf.math.top_k(A_I, self.n_ins)[1][-1]  \n",
    "        pos_index = list()\n",
    "        for i in top_pos_ids:\n",
    "            pos_index.append(i)\n",
    "\n",
    "        pos_index = tf.convert_to_tensor(pos_index)\n",
    "        top_pos = list()\n",
    "        for i in pos_index:\n",
    "            top_pos.append(h[i])\n",
    " \n",
    "        top_neg_ids = tf.math.top_k(-A_I, self.n_ins)[1][-1]\n",
    "        neg_index = list()\n",
    "        for i in top_neg_ids:\n",
    "             neg_index.append(i)\n",
    "\n",
    "        neg_index = tf.convert_to_tensor(neg_index)\n",
    "        top_neg = list()\n",
    "        for i in neg_index:\n",
    "            top_neg.append(h[i])\n",
    "\n",
    "        ins_in = tf.concat(values=[top_pos, top_neg], axis=0)\n",
    "\n",
    "        logits_in = list()\n",
    "        \n",
    "        for i in range(self.n_class * self.n_ins):\n",
    "            logit_in = tf.math.softmax(ins_classifier(ins_in[i]))\n",
    "            logits_in.append(logit_in)\n",
    "\n",
    "        return ins_label_in, logits_in\n",
    "    \n",
    "    def out_call(self, ins_classifier, h, A_O):\n",
    "        # get compressed 512-dimensional instance-level feature vectors for following use, denoted by h\n",
    "        A_O = tf.reshape(tf.convert_to_tensor(A_O), (1, len(A_O)))\n",
    "        top_pos_ids = tf.math.top_k(A_O, self.n_ins)[1][-1]\n",
    "        pos_index = list()\n",
    "        for i in top_pos_ids:\n",
    "            pos_index.append(i)\n",
    "\n",
    "        pos_index = tf.convert_to_tensor(pos_index)\n",
    "        top_pos = list()\n",
    "        for i in pos_index:\n",
    "            top_pos.append(h[i])\n",
    "\n",
    "        # mutually-exclusive -> top k instances w/ highest attention scores ==> false pos = neg\n",
    "        pos_ins_labels_out = self.generate_neg_labels(self.n_ins)\n",
    "        ins_label_out = pos_ins_labels_out\n",
    "        \n",
    "        logits_out = list()\n",
    "  \n",
    "        for i in range(self.n_ins):\n",
    "            logit_out = tf.math.softmax(ins_classifier(top_pos[i]))\n",
    "            logits_out.append(logit_out)\n",
    "\n",
    "        return ins_label_out, logits_out\n",
    "    \n",
    "    def call(self, bag_label, h, A):\n",
    "        for i in range(self.n_class):\n",
    "            ins_classifier = self.m_ins_models[i]\n",
    "            if i == bag_label:\n",
    "                A_I = list()\n",
    "                for j in range(len(A)):\n",
    "                    a_i = A[j][0][i]\n",
    "                    A_I.append(a_i)\n",
    "                ins_label_in, logits_in = self.in_call(ins_classifier, h, A_I)\n",
    "            else:\n",
    "                if self.mut_ex:\n",
    "                    A_O = list()\n",
    "                    for j in range(len(A)):\n",
    "                        a_o = A[j][0][i]\n",
    "                        A_O.append(a_o)\n",
    "                    ins_label_out, logits_out = self.out_call(ins_classifier, h, A_O)\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "        if self.mut_ex:\n",
    "            ins_labels = tf.concat(values=[ins_label_in, ins_label_out], axis=0)\n",
    "            ins_logits = logits_in + logits_out\n",
    "        else:\n",
    "            ins_labels = ins_label_in\n",
    "            ins_logits = logits_in\n",
    "        \n",
    "        return ins_labels, ins_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Bag Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1 - Single Bag Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S_Bag(tf.keras.Model):\n",
    "    def __init__(self, dim_compress_features=512, n_class=2):\n",
    "        super(S_Bag, self).__init__()\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_class = n_class\n",
    "        \n",
    "        self.s_bag_model = tf.keras.models.Sequential()\n",
    "        self.s_bag_layer = tf.keras.layers.Dense(\n",
    "            units=1, activation='linear', input_shape=(n_class, dim_compress_features), name='Bag_Classifier_Layer'\n",
    "        )\n",
    "        self.s_bag_model.add(self.s_bag_layer)\n",
    "    \n",
    "    def s_bag_classifier(self):\n",
    "        return self.s_bag_model\n",
    "    \n",
    "    def h_slide(self, A, h):\n",
    "        # compute the slide-level representation aggregated per the attention score distribution for the mth class\n",
    "        SAR = list()\n",
    "        for i in range(len(A)):\n",
    "            sar = tf.linalg.matmul(tf.transpose(A[i]), h[i])  # shape be (2,512)\n",
    "            SAR.append(sar)\n",
    "        slide_agg_rep = tf.math.add_n(SAR)   # return h_[slide,m], shape be (2,512)\n",
    "        \n",
    "        return slide_agg_rep\n",
    "        \n",
    "    def call(self, A, h):\n",
    "        slide_agg_rep = self.h_slide(A, h)\n",
    "        slide_score_unnorm = self.s_bag_model(slide_agg_rep)\n",
    "        Y_hat = tf.math.top_k(tf.reshape(slide_score_unnorm, (1, self.n_class)), 1)[1][-1]\n",
    "        Y_prob = tf.math.softmax(tf.reshape(slide_score_unnorm, (1, self.n_class)))   #shape be (1,2), predictions for each of the classes\n",
    "        \n",
    "        return slide_score_unnorm, Y_hat, Y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - List of Bag Classifier Models for Each Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_Bag(tf.keras.Model):\n",
    "    def __init__(self, dim_compress_features=512, n_class=2):\n",
    "        super(M_Bag, self).__init__()\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_class = n_class\n",
    "        \n",
    "        self.m_bag_models = list()\n",
    "        self.m_bag_model = tf.keras.models.Sequential()\n",
    "        self.m_bag_layer = tf.keras.layers.Dense(\n",
    "        units=1, activation='linear', input_shape=(n_class, dim_compress_features), name='Bag_Classifier_Layer'\n",
    "        )\n",
    "        self.m_bag_model.add(self.m_bag_layer)\n",
    "        \n",
    "        for i in range(self.n_class):\n",
    "            self.m_bag_models.append(self.m_bag_model)\n",
    "    \n",
    "    def m_bag_classifier(self):\n",
    "        return self.m_bag_models\n",
    "    \n",
    "    def h_slide(self, A, h):\n",
    "        # compute the slide-level representation aggregated per the attention score distribution for the mth class\n",
    "        SAR = list()\n",
    "        for i in range(len(A)):\n",
    "            sar = tf.linalg.matmul(tf.transpose(A[i]), h[i])  # shape be (2,512)\n",
    "            SAR.append(sar)\n",
    "        slide_agg_rep = tf.math.add_n(SAR)   # return h_[slide,m], shape be (2,512)\n",
    "        \n",
    "        return slide_agg_rep\n",
    "    \n",
    "    def call(self, A, h):\n",
    "        slide_agg_rep = self.h_slide(A, h)\n",
    "        # unnormalized slide-level score (s_[slide,m]) with uninitialized entries, shape be (1,num_of_classes)\n",
    "        slide_score_unnorm = tf.Variable(np.empty((1, self.n_class)), dtype=tf.float32)\n",
    "        # return s_[slide,m] (slide-level prediction scores)\n",
    "        for i in range(self.n_class):\n",
    "            bag_classifier = self.m_bag_models[i]\n",
    "            ssu = bag_classifier(slide_agg_rep)[i][0]\n",
    "            #print('starting printing',ssu, 'then', ssu[i][0])\n",
    "            tf.compat.v1.assign(slide_score_unnorm[0,i], ssu)\n",
    "        Y_hat = tf.math.top_k(slide_score_unnorm, 1)[1][-1]\n",
    "        Y_prob = tf.math.softmax(slide_score_unnorm)\n",
    "        \n",
    "        return slide_score_unnorm, Y_hat, Y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_CLAM(tf.keras.Model):\n",
    "    def __init__(self, att_gate=False, net_size='small', n_ins=8, n_class=2, mut_ex=False, \n",
    "                 dropout=False, drop_rate=.25, mil_ins=False, att_only=False, m_bag=False):\n",
    "        super(M_CLAM, self).__init__()\n",
    "        self.att_gate = att_gate\n",
    "        self.net_size = net_size\n",
    "        self.n_ins = n_ins\n",
    "        self.n_class = n_class\n",
    "        self.mut_ex = mut_ex\n",
    "        self.dropout = dropout\n",
    "        self.drop_rate = drop_rate\n",
    "        self.mil_ins = mil_ins\n",
    "        self.att_only = att_only\n",
    "        self.m_bag = m_bag\n",
    "        \n",
    "        self.net_shape_dict = {\n",
    "            'small': [1024, 512, 256],\n",
    "            'big': [1024, 512, 384]\n",
    "        }\n",
    "        self.net_shape = self.net_shape_dict[self.net_size]\n",
    "        \n",
    "        if self.att_gate:\n",
    "            self.att_net = G_Att_Net(dim_features=self.net_shape[0], dim_compress_features=self.net_shape[1], n_hidden_units=self.net_shape[2],\n",
    "                                    n_classes=self.n_class, dropout=self.dropout, dropout_rate=self.drop_rate)\n",
    "        else:\n",
    "            self.att_net = NG_Att_Net(dim_features=self.net_shape[0], dim_compress_features=self.net_shape[1], n_hidden_units=self.net_shape[2],\n",
    "                                    n_classes=self.n_class, dropout=self.dropout, dropout_rate=self.drop_rate)\n",
    "        \n",
    "        if self.m_bag:\n",
    "            self.bag_net = M_Bag(dim_compress_features=self.net_shape[1], n_class=self.n_class)\n",
    "        else:\n",
    "            self.bag_net = S_Bag(dim_compress_features=self.net_shape[1], n_class=self.n_class)\n",
    "        \n",
    "        self.ins_net = M_Ins(dim_compress_features=self.net_shape[1], n_class=self.n_class, n_ins=self.n_ins, mut_ex=self.mut_ex)\n",
    "        \n",
    "    def call(self, img_features, slide_label):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_features -> original 1024-dimensional instance-level feature vectors\n",
    "            slide_label -> ground-truth slide label, could be 0 or 1 for binary classification\n",
    "        \"\"\"\n",
    "\n",
    "        h, A = self.att_net.call(img_features)\n",
    "        att_score = A  # output from attention network\n",
    "        A = tf.math.softmax(A)   # softmax onattention scores \n",
    "\n",
    "        if self.att_only:\n",
    "            return att_score\n",
    "        \n",
    "        if self.mil_ins:\n",
    "            ins_labels, ins_logits = self.ins_net.call(slide_label, h, A)\n",
    "\n",
    "        slide_score_unnorm, Y_hat, Y_prob = self.bag_net.call(A, h)\n",
    "\n",
    "        return att_score, A, h, ins_labels, ins_logits, slide_score_unnorm, Y_prob, Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CLAM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = '/Users/quincy/Downloads/CLAM/train/'\n",
    "val_data = '/Users/quincy/Downloads/CLAM/valid/'\n",
    "test_data = '/Users/quincy/Downloads/CLAM/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prohibit Tensorflow Warning Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_shut_up(no_warn_op=False):\n",
    "    if no_warn_op:\n",
    "        tf.get_logger().setLevel('ERROR')\n",
    "    else:\n",
    "        print('Are you sure you want to receive the annoying TensorFlow Warning Messages?', '\\n', 'If not, check the value of your input prameter for this function and re-run it.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Optional Keras Losses, Metrics and Optimizer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lom_func():\n",
    "    losses = {\n",
    "        'Hinge': tf.keras.losses.Hinge,\n",
    "        'hinge': tf.keras.losses.hinge,\n",
    "        'SquaredHinge': tf.keras.losses.SquaredHinge,\n",
    "        'squaredhinge': tf.keras.losses.squared_hinge,\n",
    "        'CategoricalHinge': tf.keras.losses.CategoricalHinge,\n",
    "        'categoricalhinge': tf.keras.losses.categorical_hinge,\n",
    "        'BinaryCrossentropy': tf.keras.losses.BinaryCrossentropy,\n",
    "        'binarycrossentropy': tf.keras.losses.binary_crossentropy,\n",
    "        'CategoricalCrossentropy': tf.keras.losses.CategoricalCrossentropy,\n",
    "        'categoricalcrossentropy': tf.keras.losses.categorical_crossentropy\n",
    "    }\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': tf.keras.metrics.Accuracy(),\n",
    "        'BinaryAccuracy': tf.keras.metrics.BinaryAccuracy(),\n",
    "        'CategoricalAccuracy': tf.keras.metrics.CategoricalAccuracy(),\n",
    "        'Precision': tf.keras.metrics.Precision(),\n",
    "        'Recall': tf.keras.metrics.Recall(),\n",
    "        'AUC': tf.keras.metrics.AUC(),\n",
    "        'TP': tf.keras.metrics.TruePositives(),\n",
    "        'FP': tf.keras.metrics.FalsePositives(),\n",
    "        'TN': tf.keras.metrics.TrueNegatives(),\n",
    "        'FN': tf.keras.metrics.FalseNegatives(),\n",
    "        'Mean': tf.keras.metrics.Mean()\n",
    "    }\n",
    "    \n",
    "    optimizers = {\n",
    "        'Adam': tf.keras.optimizers.Adam,\n",
    "        'Adamx': tf.keras.optimizers.Adamax,\n",
    "        'AdamW': tfa.optimizers.AdamW\n",
    "    }\n",
    "    \n",
    "    return losses, metrics, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, metrics, optimizers = lom_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CLAM Model on the Given Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(i_model, b_model, c_model, train_path, i_loss_func, b_loss_func, mutual_ex=False, \n",
    "               n_class=2, c1=0.7, c2=0.3, learn_rate=2e-04, l2_decay=1e-05):\n",
    "    loss_total = list(); loss_ins = list(); loss_bag = list(); acc = list(); auc = list(); precision = list(); recall = list()\n",
    "    c_optimizer = optimizers['AdamW'](learning_rate=learn_rate, weight_decay=l2_decay)\n",
    "    i_optimizer = optimizers['AdamW'](learning_rate=learn_rate, weight_decay=l2_decay)\n",
    "    b_optimizer = optimizers['AdamW'](learning_rate=0.001, weight_decay=l2_decay)\n",
    "    \n",
    "    for i in os.listdir(train_path):\n",
    "        print('=', end = \"\")\n",
    "        single_train_data = train_path + i\n",
    "        img_features, slide_label, slide_true = get_data_from_tf(single_train_data)\n",
    "        train_tp = 0; train_fp = 0; train_tn = 0; train_fn = 0; train_acc = 0.0; train_auc = 0.0; train_precision = 0.0; train_recall = 0.0\n",
    "  \n",
    "        with tf.GradientTape() as i_tape, tf.GradientTape() as b_tape, tf.GradientTape() as c_tape:\n",
    "            att_score, A, h, ins_labels, ins_logits, slide_score_unnorm, Y_prob, Y_hat = c_model.call(img_features, slide_label)\n",
    "            ins_labels, ins_logits = i_model.call(slide_label, h, A)\n",
    "            slide_score_unnorm, Y_hat, Y_prob = b_model.call(A, h)\n",
    "            ins_loss = list()\n",
    "            for i in range(len(ins_logits)):\n",
    "                i_loss = i_loss_func(tf.one_hot(ins_labels[i], 2), ins_logits[i])\n",
    "                ins_loss.append(i_loss)\n",
    "                #print('ins logit', ins_logits[i], 'label', tf.one_hot(ins_labels[i],2), 'loss', i_loss)\n",
    "            if mutual_ex:\n",
    "                I_Loss = (tf.math.add_n(ins_loss) / len(ins_logits)) / n_class\n",
    "            else:\n",
    "                I_Loss = tf.math.add_n(ins_loss) / len(ins_logits)   \n",
    "            B_Loss = b_loss_func(slide_true, Y_prob)\n",
    "            T_Loss = c2*B_Loss + c1*I_Loss   \n",
    "            #print('unnorm', slide_score_unnorm, 'b logit,', Y_prob, 'label', slide_true, 'loss', B_Loss,'\\n')\n",
    "            #print('loss', T_Loss)\n",
    "        i_grad = i_tape.gradient(I_Loss, i_model.trainable_variables)\n",
    "        #print('grad', i_grad)\n",
    "        i_optimizer.apply_gradients(zip(i_grad, i_model.trainable_variables))\n",
    "        b_grad = b_tape.gradient(B_Loss, b_model.trainable_variables)\n",
    "        #print('grad', b_grad)\n",
    "        b_optimizer.apply_gradients(zip(b_grad, b_model.trainable_variables))\n",
    "        c_grad = c_tape.gradient(T_Loss, c_model.trainable_variables)\n",
    "        c_optimizer.apply_gradients(zip(c_grad, c_model.trainable_variables)) \n",
    "        \n",
    "        loss_total.append(T_Loss); loss_ins.append(I_Loss); loss_bag.append(B_Loss); tp = metrics['TP'](slide_true, Y_prob); \\\n",
    "        fp = metrics['FP'](slide_true, Y_prob); tn = metrics['TN'](slide_true, Y_prob); fn = metrics['FN'](slide_true, Y_prob); \\\n",
    "        acc_value = metrics['BinaryAccuracy'](slide_true, Y_prob); auc_value = metrics['AUC'](slide_true, Y_prob); \\\n",
    "        precision_value = metrics['Precision'](slide_true, Y_prob); recall_value = metrics['Recall'](slide_true, Y_prob); \\\n",
    "        train_tp += tp; train_fp += fp; train_tn += tn; train_fn += fn; train_acc += acc_value; train_auc += auc_value; \\\n",
    "        train_precision += precision_value; train_recall += recall_value; acc.append(train_acc); auc.append(train_auc); \\\n",
    "        precision.append(train_precision); recall.append(train_recall)\n",
    "        \n",
    "    acc_train = tf.math.add_n(acc) / len(os.listdir(train_path)); auc_train = tf.math.add_n(auc) / len(os.listdir(train_path)); \\\n",
    "    precision_train = tf.math.add_n(precision) / len(os.listdir(train_path)); recall_train = tf.math.add_n(recall) / len(os.listdir(train_path)); \\\n",
    "    train_loss = tf.math.add_n(loss_total) / len(os.listdir(train_path)); train_ins_loss = tf.math.add_n(loss_ins) / len(os.listdir(train_path)); \\\n",
    "    train_bag_loss = tf.math.add_n(loss_bag) / len(os.listdir(train_path))\n",
    "\n",
    "    return train_loss, train_ins_loss, train_bag_loss, acc_train, auc_train, train_tp, train_fp, train_tn, train_fn, precision_train, recall_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(c_model, val_path, i_loss_func, b_loss_func, mutual_ex=False, n_class=2, c1=0.7, c2=0.3): \n",
    "    loss_t = list(); loss_i = list(); loss_b = list(); acc = list(); auc = list(); precision = list(); recall = list()\n",
    "    \n",
    "    for j in os.listdir(val_path):\n",
    "        print('=', end = \"\")\n",
    "        single_val_data = val_path + j\n",
    "        img_features, slide_label, slide_true = get_data_from_tf(single_val_data)\n",
    "\n",
    "        att_score, A, h, ins_labels, ins_logits, slide_score_unnorm, Y_prob, Y_hat = c_model.call(img_features, slide_label)\n",
    "        ins_loss = list()\n",
    "        for i in range(len(ins_logits)):\n",
    "            i_loss = i_loss_func(tf.one_hot(ins_labels[i], 2), ins_logits[i])\n",
    "            ins_loss.append(i_loss)\n",
    "        if mutual_ex:\n",
    "            I_Loss = (tf.math.add_n(ins_loss) / len(ins_logits)) / n_class\n",
    "        else:\n",
    "            I_Loss = tf.math.add_n(ins_loss) / len(ins_logits)\n",
    "        B_Loss = b_loss_func(slide_true, Y_prob)\n",
    "        T_Loss = c2*B_Loss + c1*I_Loss\n",
    "        \n",
    "        loss_t.append(T_Loss); loss_i.append(I_Loss); loss_b.append(B_Loss)\n",
    "          \n",
    "        val_tp = 0; val_fp = 0; val_tn = 0; val_fn = 0; val_acc = 0.0; val_auc = 0.0; val_precision = 0.0; val_recall = 0.0\n",
    "        \n",
    "        tp = metrics['TP'](slide_true, Y_prob); fp = metrics['FP'](slide_true, Y_prob); tn = metrics['TN'](slide_true, Y_prob); \\\n",
    "        fn = metrics['FN'](slide_true, Y_prob)\n",
    "        acc_value = metrics['BinaryAccuracy'](slide_true, Y_prob); auc_value = metrics['AUC'](slide_true, Y_prob); \\\n",
    "        precision_value = metrics['Precision'](slide_true, Y_prob); recall_value = metrics['Recall'](slide_true, Y_prob)\n",
    "    \n",
    "        val_tp += tp; val_fp += fp; val_tn += tn; val_fn += fn; val_acc += acc_value; val_auc += auc_value; \\\n",
    "        val_precision += precision_value; val_recall += recall_value\n",
    "        \n",
    "        acc.append(val_acc); auc.append(val_auc); precision.append(val_precision); recall.append(val_recall); loss_t.append(T_Loss);\n",
    "\n",
    "    val_loss = tf.math.add_n(loss_t) / len(os.listdir(val_path)); val_ins_loss = tf.math.add_n(loss_i) / len(os.listdir(val_path)); \\\n",
    "    val_bag_loss = tf.math.add_n(loss_b) / len(os.listdir(val_path)); val_acc = tf.math.add_n(acc) / len(os.listdir(val_path)); \\\n",
    "    val_auc = tf.math.add_n(auc) / len(os.listdir(val_path)); val_precision = tf.math.add_n(precision) / len(os.listdir(val_path)); \\\n",
    "    val_recall = tf.math.add_n(recall) / len(os.listdir(val_path))\n",
    "  \n",
    "    return val_loss, val_ins_loss, val_bag_loss, val_acc, val_auc, val_tp, val_fp, val_tn, val_fn, val_precision, val_recall    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = NG_Att_Net(dim_features=1024, dim_compress_features=512, n_hidden_units=256, n_classes=2,\n",
    "                 dropout=False, dropout_rate=.25)\n",
    "g = G_Att_Net(dim_features=1024, dim_compress_features=512, n_hidden_units=256, n_classes=2,\n",
    "                 dropout=False, dropout_rate=.25)\n",
    "\n",
    "m_ins = M_Ins(dim_compress_features=512, n_class=2, n_ins=8, mut_ex=True)\n",
    "\n",
    "s_bag = S_Bag(dim_compress_features=512, n_class=2)\n",
    "m_bag = M_Bag(dim_compress_features=512, n_class=2)\n",
    "\n",
    "m_clam = M_CLAM(att_gate=True, net_size='small', n_ins=8, n_class=2, mut_ex=False, \n",
    "                 dropout=False, drop_rate=.25, mil_ins=True, att_only=False, m_bag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = '/Users/quincy/Downloads/CLAM/log/' + current_time + '/train'\n",
    "val_log_dir = '/Users/quincy/Downloads/CLAM/log/' + current_time + '/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(train_log, val_log, epochs=1):\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log)\n",
    "    val_summary_writer = tf.summary.create_file_writer(val_log)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Step\n",
    "        start_time = time.time()\n",
    "        train_loss, train_ins_loss, train_bag_loss, acc_train, auc_train, train_tp, train_fp, \\\n",
    "        train_tn, train_fn, precision_train, recall_train = train_step(\n",
    "            i_model=m_ins, b_model=s_bag, c_model=m_clam, train_path=train_data, i_loss_func=losses['hinge'], b_loss_func=losses['binarycrossentropy'], \n",
    "            mutual_ex=True, n_class=2, c1=0.7, c2=0.3, learn_rate=2e-04, l2_decay=1e-05\n",
    "        )\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('Total Loss', float(train_loss), step=epoch)\n",
    "            tf.summary.scalar('Instance Loss', float(train_ins_loss), step=epoch)\n",
    "            tf.summary.scalar('Bag Loss', float(train_bag_loss), step=epoch)\n",
    "            tf.summary.scalar('Accuracy', float(acc_train), step=epoch)\n",
    "            tf.summary.scalar('AUC', float(auc_train), step=epoch)\n",
    "            tf.summary.scalar('Precision', float(precision_train), step=epoch)\n",
    "            tf.summary.scalar('Recall', float(recall_train), step=epoch)\n",
    "            tf.summary.histogram('True Positive', int(train_tp), step=epoch)\n",
    "            tf.summary.histogram('False Positive', int(train_fp), step=epoch)\n",
    "            tf.summary.histogram('True Negative', int(train_tn), step=epoch)\n",
    "            tf.summary.histogram('False Negative', int(train_fn), step=epoch)\n",
    "        # Validation Step\n",
    "        val_loss, val_ins_loss, val_bag_loss, val_acc, val_auc, val_tp, val_fp, val_tn, \\\n",
    "        val_fn, val_precision, val_recall = val_step(\n",
    "            c_model=m_clam, val_path=val_data, i_loss_func=losses['hinge'], \n",
    "            b_loss_func=losses['binarycrossentropy'], mutual_ex=True, n_class=2, c1=0.7, c2=0.3\n",
    "        )  \n",
    "        with val_summary_writer.as_default():\n",
    "            tf.summary.scalar('Total Loss', float(val_loss), step=epoch)\n",
    "            tf.summary.scalar('Instance Loss', float(val_ins_loss), step=epoch)\n",
    "            tf.summary.scalar('Bag Loss', float(val_bag_loss), step=epoch)\n",
    "            tf.summary.scalar('Accuracy', float(val_acc), step=epoch)\n",
    "            tf.summary.scalar('AUC', float(val_auc), step=epoch)\n",
    "            tf.summary.scalar('Precision', float(val_precision), step=epoch)\n",
    "            tf.summary.scalar('Recall', float(val_recall), step=epoch)\n",
    "            tf.summary.histogram('True Positive', int(val_tp), step=epoch)\n",
    "            tf.summary.histogram('False Positive', int(val_fp), step=epoch)\n",
    "            tf.summary.histogram('True Negative', int(val_tn), step=epoch)\n",
    "            tf.summary.histogram('False Negative', int(val_fn), step=epoch)\n",
    "        epoch_run_time = time.time() - start_time\n",
    "        template = '\\n Epoch {},  Train Loss: {}, Train Accuracy: {}, Val Loss: {}, Val Accuracy: {}, Epoch Running Time: {}'\n",
    "        print(template.format(epoch + 1, \n",
    "                              f\"{float(train_loss):.8}\", \n",
    "                              f\"{float(acc_train):.4%}\",\n",
    "                              f\"{float(val_loss):.8}\", \n",
    "                              f\"{float(val_acc):.4%}\", \n",
    "                              \"--- %s mins ---\" % int(epoch_run_time / 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_shut_up(no_warn_op=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval(train_log=train_log_dir, val_log=val_log_dir, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

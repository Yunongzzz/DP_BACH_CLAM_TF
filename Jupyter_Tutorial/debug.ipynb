{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import PIL\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import statistics\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Self-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_tf(tf_path):\n",
    "    feature = {'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "               'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "               'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "               'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "               'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "               'image_name': tf.io.FixedLenFeature([], tf.string),\n",
    "               'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "               'image_feature': tf.io.FixedLenFeature([], tf.string)}\n",
    "\n",
    "    tfrecord_dataset = tf.data.TFRecordDataset(tf_path)\n",
    "\n",
    "    def _parse_image_function(key):\n",
    "        return tf.io.parse_single_example(key, feature)\n",
    "\n",
    "    CLAM_dataset = tfrecord_dataset.map(_parse_image_function)\n",
    "\n",
    "    image_features = list()\n",
    "\n",
    "    for tfrecord_value in CLAM_dataset:\n",
    "        img_feature = tf.io.parse_tensor(tfrecord_value['image_feature'], 'float32')\n",
    "        slide_labels = tfrecord_value['label']\n",
    "        slide_label = int(slide_labels)\n",
    "        image_features.append(img_feature)\n",
    "\n",
    "    return image_features, slide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List):\n",
    "    mf = max(set(List), key=List.count)\n",
    "    return mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_shut_up(no_warn_op=False):\n",
    "    if no_warn_op:\n",
    "        tf.get_logger().setLevel('ERROR')\n",
    "    else:\n",
    "        print('Are you sure you want to receive the annoying TensorFlow Warning Messages?', \\\n",
    "              '\\n', 'If not, check the value of your input prameter for this function and re-run it.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CLAM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import None-Gated Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NG_Att_Net(tf.keras.Model):\n",
    "    def __init__(self, dim_features=1024, dim_compress_features=512, n_hidden_units=256, n_classes=2,\n",
    "                 dropout=False, dropout_rate=.25):\n",
    "        super(NG_Att_Net, self).__init__()\n",
    "        self.dim_features = dim_features\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.compression_model = tf.keras.models.Sequential()\n",
    "        self.model = tf.keras.models.Sequential()\n",
    "\n",
    "        self.fc_compress_layer = tf.keras.layers.Dense(units=dim_compress_features, activation='relu',\n",
    "                                                       input_shape=(dim_features,), kernel_initializer='glorot_normal',\n",
    "                                                       bias_initializer='zeros', name='Fully_Connected_Layer')\n",
    "\n",
    "        self.compression_model.add(self.fc_compress_layer)\n",
    "        self.model.add(self.fc_compress_layer)\n",
    "\n",
    "        self.att_layer1 = tf.keras.layers.Dense(units=n_hidden_units, activation='tanh',\n",
    "                                                input_shape=(dim_compress_features,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer1')\n",
    "\n",
    "        self.att_layer2 = tf.keras.layers.Dense(units=n_classes, activation='linear', input_shape=(n_hidden_units,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer2')\n",
    "\n",
    "        self.model.add(self.att_layer1)\n",
    "\n",
    "        if dropout:\n",
    "            self.model.add(tf.keras.layers.Dropout(dropout_rate, name='Dropout_Layer'))\n",
    "\n",
    "        self.model.add(self.att_layer2)\n",
    "\n",
    "    def att_model(self):\n",
    "        attention_model = [self.compression_model, self.model]\n",
    "        return attention_model\n",
    "\n",
    "    def call(self, x):\n",
    "        h = list()\n",
    "        A = list()\n",
    "        \n",
    "        for i in x:\n",
    "            c_imf = self.att_model()[0](i)\n",
    "            h.append(c_imf)\n",
    "        \n",
    "        for i in x:\n",
    "            a = self.att_model()[1](i)\n",
    "            A.append(a)\n",
    "        return h, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Gated Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_Att_Net(tf.keras.Model):\n",
    "    def __init__(self, dim_features=1024, dim_compress_features=512, n_hidden_units=256, n_classes=2,\n",
    "                 dropout=False, dropout_rate=.25):\n",
    "        super(G_Att_Net, self).__init__()\n",
    "        self.dim_features = dim_features\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.compression_model = tf.keras.models.Sequential()\n",
    "        self.model1 = tf.keras.models.Sequential()\n",
    "        self.model2 = tf.keras.models.Sequential()\n",
    "        self.model = tf.keras.models.Sequential()\n",
    "\n",
    "        self.fc_compress_layer = tf.keras.layers.Dense(units=dim_compress_features, activation='relu',\n",
    "                                                       input_shape=(dim_features,), kernel_initializer='glorot_normal',\n",
    "                                                       bias_initializer='zeros', name='Fully_Connected_Layer')\n",
    "\n",
    "        self.compression_model.add(self.fc_compress_layer)\n",
    "        self.model1.add(self.fc_compress_layer)\n",
    "        self.model2.add(self.fc_compress_layer)\n",
    "\n",
    "        self.att_layer1 = tf.keras.layers.Dense(units=n_hidden_units, activation='tanh', input_shape=(dim_features,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer1')\n",
    "\n",
    "        self.att_layer2 = tf.keras.layers.Dense(units=n_hidden_units, activation='sigmoid', input_shape=(dim_features,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer2')\n",
    "\n",
    "        self.att_layer3 = tf.keras.layers.Dense(units=n_classes, activation='linear', input_shape=(n_hidden_units,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer3')\n",
    "\n",
    "        self.model1.add(self.att_layer1)\n",
    "        self.model2.add(self.att_layer2)\n",
    "\n",
    "        if dropout:\n",
    "            self.model1.add(tf.keras.layers.Dropout(dropout_rate, name='Dropout_Layer'))\n",
    "            self.model2.add(tf.keras.layers.Dropout(dropout_rate, name='Dropout_Layer'))\n",
    "\n",
    "        self.model.add(self.att_layer3)\n",
    "\n",
    "    def att_model(self):\n",
    "        attention_model = [self.compression_model, self.model1, self.model2, self.model]\n",
    "        return attention_model\n",
    "\n",
    "    def call(self, x):\n",
    "        h = list()\n",
    "        A = list()\n",
    "        \n",
    "        for i in x:\n",
    "            c_imf = self.att_model()[0](i)\n",
    "            h.append(c_imf)\n",
    "            \n",
    "        for i in x:\n",
    "            layer1_output = self.att_model()[1](i)  \n",
    "            layer2_output = self.att_model()[2](i)  \n",
    "            a = tf.math.multiply(layer1_output, layer2_output)  \n",
    "            a = self.att_model()[3](a)  \n",
    "            A.append(a)\n",
    "\n",
    "        return h, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Instance Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ins(tf.keras.Model):\n",
    "    def __init__(self, dim_compress_features=512, n_class=2, n_ins=8, mut_ex=False):\n",
    "        super(Ins, self).__init__()\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_class = n_class\n",
    "        self.n_ins = n_ins\n",
    "        self.mut_ex = mut_ex\n",
    "\n",
    "        self.ins_model = list()\n",
    "        self.m_ins_model = tf.keras.models.Sequential()\n",
    "        self.m_ins_layer = tf.keras.layers.Dense(\n",
    "            units=self.n_class, activation='linear', input_shape=(self.dim_compress_features,),\n",
    "            name='Instance_Classifier_Layer'\n",
    "        )\n",
    "        self.m_ins_model.add(self.m_ins_layer)\n",
    "\n",
    "        for i in range(self.n_class):\n",
    "            self.ins_model.append(self.m_ins_model)\n",
    "\n",
    "    def ins_classifier(self):\n",
    "        return self.ins_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_pos_labels(n_pos_sample):\n",
    "        return tf.fill(dims=[n_pos_sample, ], value=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_neg_labels(n_neg_sample):\n",
    "        return tf.fill(dims=[n_neg_sample, ], value=0)\n",
    "    \n",
    "    def in_call(self, ins_classifier, h, A_I):\n",
    "        pos_label = self.generate_pos_labels(self.n_ins)\n",
    "        neg_label = self.generate_neg_labels(self.n_ins)\n",
    "        ins_label_in = tf.concat(values=[pos_label, neg_label], axis=0)\n",
    "        A_I = tf.reshape(tf.convert_to_tensor(A_I), (1, len(A_I))) \n",
    "        \n",
    "        top_pos_ids = tf.math.top_k(A_I, self.n_ins)[1][-1]  \n",
    "        pos_index = list()\n",
    "        for i in top_pos_ids:\n",
    "            pos_index.append(i)\n",
    "\n",
    "        pos_index = tf.convert_to_tensor(pos_index)\n",
    "        top_pos = list()\n",
    "        for i in pos_index:\n",
    "            top_pos.append(h[i])\n",
    " \n",
    "        top_neg_ids = tf.math.top_k(-A_I, self.n_ins)[1][-1]\n",
    "        neg_index = list()\n",
    "        for i in top_neg_ids:\n",
    "             neg_index.append(i)\n",
    "\n",
    "        neg_index = tf.convert_to_tensor(neg_index)\n",
    "        top_neg = list()\n",
    "        for i in neg_index:\n",
    "            top_neg.append(h[i])\n",
    "\n",
    "        ins_in = tf.concat(values=[top_pos, top_neg], axis=0)\n",
    "        logits_unnorm_in = list()\n",
    "        logits_in = list()\n",
    "        \n",
    "        for i in range(self.n_class * self.n_ins):\n",
    "            ins_score_unnorm_in = ins_classifier(ins_in[i])\n",
    "            logit_in = tf.math.softmax(ins_score_unnorm_in)\n",
    "            logits_unnorm_in.append(ins_score_unnorm_in)\n",
    "            logits_in.append(logit_in)\n",
    "\n",
    "        return ins_label_in, logits_unnorm_in, logits_in\n",
    "    \n",
    "    def out_call(self, ins_classifier, h, A_O):\n",
    "        # get compressed 512-dimensional instance-level feature vectors for following use, denoted by h\n",
    "        A_O = tf.reshape(tf.convert_to_tensor(A_O), (1, len(A_O)))\n",
    "        top_pos_ids = tf.math.top_k(A_O, self.n_ins)[1][-1]\n",
    "        pos_index = list()\n",
    "        for i in top_pos_ids:\n",
    "            pos_index.append(i)\n",
    "\n",
    "        pos_index = tf.convert_to_tensor(pos_index)\n",
    "        top_pos = list()\n",
    "        for i in pos_index:\n",
    "            top_pos.append(h[i])\n",
    "\n",
    "        # mutually-exclusive -> top k instances w/ highest attention scores ==> false pos = neg\n",
    "        pos_ins_labels_out = self.generate_neg_labels(self.n_ins)\n",
    "        ins_label_out = pos_ins_labels_out\n",
    "        \n",
    "        logits_unnorm_out = list()\n",
    "        logits_out = list()\n",
    "  \n",
    "        for i in range(self.n_ins):\n",
    "            ins_score_unnorm_out = ins_classifier(top_pos[i])\n",
    "            logit_out = tf.math.softmax(ins_score_unnorm_out)\n",
    "            logits_unnorm_out.append(ins_score_unnorm_out)\n",
    "            logits_out.append(logit_out)\n",
    "\n",
    "        return ins_label_out, logits_unnorm_out, logits_out\n",
    "    \n",
    "    def call(self, bag_label, h, A):\n",
    "        for i in range(self.n_class):\n",
    "            ins_classifier = self.ins_classifier()[i]\n",
    "            if i == bag_label:\n",
    "                A_I = list()\n",
    "                for j in range(len(A)):\n",
    "                    a_i = A[j][0][i]\n",
    "                    A_I.append(a_i)\n",
    "                ins_label_in, logits_unnorm_in, logits_in = self.in_call(ins_classifier, h, A_I)\n",
    "            else:\n",
    "                if self.mut_ex:\n",
    "                    A_O = list()\n",
    "                    for j in range(len(A)):\n",
    "                        a_o = A[j][0][i]\n",
    "                        A_O.append(a_o)\n",
    "                    ins_label_out, logits_unnorm_out, logits_out = self.out_call(ins_classifier, h, A_O)\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "        if self.mut_ex:\n",
    "            ins_labels = tf.concat(values=[ins_label_in, ins_label_out], axis=0)\n",
    "            ins_logits_unnorm = logits_unnorm_in + logits_unnorm_out\n",
    "            ins_logits = logits_in + logits_out\n",
    "        else:\n",
    "            ins_labels = ins_label_in\n",
    "            ins_logits_unnorm = logits_unnorm_in\n",
    "            ins_logits = logits_in\n",
    "        \n",
    "        return ins_labels, ins_logits_unnorm, ins_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Bag Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S_Bag(tf.keras.Model):\n",
    "    def __init__(self, dim_compress_features=512, n_class=2):\n",
    "        super(S_Bag, self).__init__()\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_class = n_class\n",
    "\n",
    "        self.s_bag_model = tf.keras.models.Sequential()\n",
    "        self.s_bag_layer = tf.keras.layers.Dense(\n",
    "            units=1, activation='linear', input_shape=(self.n_class, self.dim_compress_features),\n",
    "            name='Bag_Classifier_Layer'\n",
    "        )\n",
    "        self.s_bag_model.add(self.s_bag_layer)\n",
    "\n",
    "    def bag_classifier(self):\n",
    "        return self.s_bag_model\n",
    "\n",
    "    def h_slide(self, A, h):\n",
    "        # compute the slide-level representation aggregated per the attention score distribution for the mth class\n",
    "        SAR = list()\n",
    "        for i in range(len(A)):\n",
    "            sar = tf.linalg.matmul(tf.transpose(A[i]), h[i])  # shape be (2,512)\n",
    "            SAR.append(sar)\n",
    "        slide_agg_rep = tf.math.add_n(SAR)   # return h_[slide,m], shape be (2,512)\n",
    "        \n",
    "        return slide_agg_rep\n",
    "    \n",
    "    def call(self, bag_label, A, h):\n",
    "        slide_agg_rep = self.h_slide(A, h)\n",
    "        bag_classifier = self.bag_classifier()\n",
    "        slide_score_unnorm = bag_classifier(slide_agg_rep)\n",
    "        slide_score_unnorm = tf.reshape(slide_score_unnorm, (1, self.n_class))\n",
    "        Y_hat = tf.math.top_k(slide_score_unnorm ,1)[1][-1]\n",
    "        Y_prob = tf.math.softmax(tf.reshape(slide_score_unnorm, (1, self.n_class)))   #shape be (1,2), predictions for each of the classes\n",
    "        predict_slide_label = np.argmax(Y_prob.numpy())\n",
    "        \n",
    "        Y_true = tf.one_hot([bag_label], 2)\n",
    "\n",
    "        return slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_Bag(tf.keras.Model):\n",
    "    def __init__(self, dim_compress_features=512, n_class=2):\n",
    "        super(M_Bag, self).__init__()\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_class = n_class\n",
    "\n",
    "        self.m_bag_models = list()\n",
    "        self.m_bag_model = tf.keras.models.Sequential() \n",
    "        self.m_bag_layer = tf.keras.layers.Dense(\n",
    "            units = 1, activation = 'linear', input_shape=(self.dim_compress_features,), name = 'Bag_Classifier_Layer'\n",
    "        )\n",
    "        self.m_bag_model.add(self.m_bag_layer)\n",
    "        for i in range(self.n_class):\n",
    "            self.m_bag_models.append(self.m_bag_model)\n",
    "            \n",
    "    def bag_classifier(self):       \n",
    "        return self.m_bag_models\n",
    "\n",
    "    def h_slide(self, A, h):\n",
    "        # compute the slide-level representation aggregated per the attention score distribution for the mth class\n",
    "        SAR = list()\n",
    "        for i in range(len(A)):\n",
    "            sar = tf.linalg.matmul(tf.transpose(A[i]), h[i])  # shape be (2,512)\n",
    "            SAR.append(sar)\n",
    "        slide_agg_rep = tf.math.add_n(SAR)  # return h_[slide,m], shape be (2,512)\n",
    "\n",
    "        return slide_agg_rep\n",
    "\n",
    "    def in_call(self, bag_classifier, h_slide_I):\n",
    "        ssu_in = bag_classifier(h_slide_I)[0][0]\n",
    "\n",
    "        return ssu_in\n",
    "    \n",
    "    def out_call(self, bag_classifier, h_slide_O):\n",
    "        ssu_out = bag_classifier(h_slide_O)[0][0]\n",
    "        \n",
    "        return ssu_out\n",
    "    \n",
    "    def call(self, bag_label, A, h):\n",
    "        slide_agg_rep = self.h_slide(A, h)\n",
    "        # unnormalized slide-level score (s_[slide,m]) with uninitialized entries, shape be (1,num_of_classes)\n",
    "        slide_score_unnorm = tf.Variable(np.empty((1, self.n_class)), dtype=tf.float32)\n",
    "        slide_score_unnorm = tf.reshape(slide_score_unnorm, (1, self.n_class)).numpy()\n",
    " \n",
    "        # return s_[slide,m] (slide-level prediction scores)\n",
    "        for i in range(self.n_class):\n",
    "            bag_classifier = self.bag_classifier()[i]\n",
    "            if i == bag_label:\n",
    "                h_slide_I = tf.reshape(slide_agg_rep[i], (1, self.dim_compress_features))\n",
    "                ssu_in = self.in_call(bag_classifier, h_slide_I)\n",
    "            else:\n",
    "                h_slide_O = tf.reshape(slide_agg_rep[i], (1, self.dim_compress_features))\n",
    "                ssu_out = self.out_call(bag_classifier, h_slide_O)\n",
    "                \n",
    "        for i in range(self.n_class):\n",
    "            if i == bag_label:\n",
    "                slide_score_unnorm[0, i] = ssu_in\n",
    "            else:\n",
    "                slide_score_unnorm[0, i] = ssu_out\n",
    "        slide_score_unnorm = tf.convert_to_tensor(slide_score_unnorm)\n",
    "\n",
    "        Y_hat = tf.math.top_k(slide_score_unnorm, 1)[1][-1]\n",
    "        Y_prob = tf.math.softmax(slide_score_unnorm)\n",
    "        predict_slide_label = np.argmax(Y_prob.numpy())\n",
    "        \n",
    "        Y_true = tf.one_hot([bag_label], 2)\n",
    "\n",
    "        return slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S_CLAM(tf.keras.Model):\n",
    "    def __init__(self, att_gate=False, net_size='small', n_ins=8, n_class=2, mut_ex=False, \n",
    "                 dropout=False, drop_rate=.25, mil_ins=False, att_only=False):\n",
    "        super(S_CLAM, self).__init__()\n",
    "        self.att_gate = att_gate\n",
    "        self.net_size = net_size\n",
    "        self.n_ins = n_ins\n",
    "        self.n_class = n_class\n",
    "        self.mut_ex = mut_ex\n",
    "        self.dropout = dropout\n",
    "        self.drop_rate = drop_rate\n",
    "        self.mil_ins = mil_ins\n",
    "        self.att_only = att_only\n",
    "        \n",
    "        self.net_shape_dict = {\n",
    "            'small': [1024, 512, 256],\n",
    "            'big': [1024, 512, 384]\n",
    "        }\n",
    "        self.net_shape = self.net_shape_dict[self.net_size]\n",
    "        \n",
    "        if self.att_gate:\n",
    "            self.att_net = G_Att_Net(dim_features=self.net_shape[0], dim_compress_features=self.net_shape[1], n_hidden_units=self.net_shape[2],\n",
    "                                    n_classes=self.n_class, dropout=self.dropout, dropout_rate=self.drop_rate)\n",
    "        else:\n",
    "            self.att_net = NG_Att_Net(dim_features=self.net_shape[0], dim_compress_features=self.net_shape[1], n_hidden_units=self.net_shape[2],\n",
    "                                    n_classes=self.n_class, dropout=self.dropout, dropout_rate=self.drop_rate)\n",
    "        \n",
    "        self.ins_net = Ins(dim_compress_features=self.net_shape[1], n_class=self.n_class, n_ins=self.n_ins, mut_ex=self.mut_ex)\n",
    "        \n",
    "        self.bag_net = S_Bag(dim_compress_features=self.net_shape[1], n_class=self.n_class)\n",
    "        \n",
    "    def clam_model(self):\n",
    "        att_model = self.att_net.att_model()\n",
    "        ins_classifier = self.ins_net.ins_classifier()\n",
    "        bag_classifier = self.bag_net.bag_classifier()\n",
    "        \n",
    "        clam_model = [att_model, ins_classifier, bag_classifier]\n",
    "        \n",
    "        return clam_model\n",
    "\n",
    "    def call(self, img_features, slide_label):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_features -> original 1024-dimensional instance-level feature vectors\n",
    "            slide_label -> ground-truth slide label, could be 0 or 1 for binary classification\n",
    "        \"\"\"\n",
    "\n",
    "        h, A = self.att_net.call(img_features)\n",
    "        att_score = A  # output from attention network\n",
    "        A = tf.math.softmax(A)   # softmax onattention scores \n",
    "\n",
    "        if self.att_only:\n",
    "            return att_score\n",
    "        \n",
    "        if self.mil_ins:\n",
    "            ins_labels, ins_logits_unnorm, ins_logits = self.ins_net.call(slide_label, h, A)\n",
    "\n",
    "        slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true = self.bag_net.call(slide_label, A, h)\n",
    "\n",
    "        return att_score, A, h, ins_labels, ins_logits_unnorm, ins_logits, slide_score_unnorm, Y_prob, Y_hat, Y_true, predict_slide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_CLAM(tf.keras.Model):\n",
    "    def __init__(self, att_gate=False, net_size='small', n_ins=8, n_class=2, mut_ex=False,\n",
    "                 dropout=False, drop_rate=.25, mil_ins=False, att_only=False):\n",
    "        super(M_CLAM, self).__init__()\n",
    "        self.att_gate = att_gate\n",
    "        self.net_size = net_size\n",
    "        self.n_ins = n_ins\n",
    "        self.n_class = n_class\n",
    "        self.mut_ex = mut_ex\n",
    "        self.dropout = dropout\n",
    "        self.drop_rate = drop_rate\n",
    "        self.mil_ins = mil_ins\n",
    "        self.att_only = att_only\n",
    "\n",
    "        self.net_shape_dict = {\n",
    "            'small': [1024, 512, 256],\n",
    "            'big': [1024, 512, 384]\n",
    "        }\n",
    "        self.net_shape = self.net_shape_dict[self.net_size]\n",
    "\n",
    "        if self.att_gate:\n",
    "            self.att_net = G_Att_Net(dim_features=self.net_shape[0], dim_compress_features=self.net_shape[1],\n",
    "                                     n_hidden_units=self.net_shape[2], n_classes=self.n_class, \n",
    "                                     dropout=self.dropout, dropout_rate=self.drop_rate)\n",
    "        else:\n",
    "            self.att_net = NG_Att_Net(dim_features=self.net_shape[0], dim_compress_features=self.net_shape[1],\n",
    "                                      n_hidden_units=self.net_shape[2], n_classes=self.n_class, \n",
    "                                      dropout=self.dropout, dropout_rate=self.drop_rate)\n",
    "\n",
    "        self.ins_net = Ins(dim_compress_features=self.net_shape[1], n_class=self.n_class, \n",
    "                           n_ins=self.n_ins, mut_ex=self.mut_ex)\n",
    "        \n",
    "        self.bag_net = M_Bag(dim_compress_features=self.net_shape[1], n_class=self.n_class)\n",
    "        \n",
    "    def clam_model(self):\n",
    "        att_model = self.att_net.att_model()\n",
    "        ins_classifier = self.ins_net.ins_classifier()\n",
    "        bag_classifier = self.bag_net.bag_classifier()\n",
    "        \n",
    "        clam_model = [att_model, ins_classifier, bag_classifier]\n",
    "        \n",
    "        return clam_model\n",
    "    \n",
    "    def call(self, img_features, slide_label):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_features -> original 1024-dimensional instance-level feature vectors\n",
    "            slide_label -> ground-truth slide label, could be 0 or 1 for binary classification\n",
    "        \"\"\"\n",
    "\n",
    "        h, A = self.att_net.call(img_features)\n",
    "        att_score = A  # output from attention network\n",
    "        A = tf.math.softmax(A)  # softmax onattention scores\n",
    "\n",
    "        if self.att_only:\n",
    "            return att_score\n",
    "\n",
    "        if self.mil_ins:\n",
    "            ins_labels, ins_logits_unnorm, ins_logits = self.ins_net.call(slide_label, h, A)\n",
    "\n",
    "        slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true = self.bag_net.call(slide_label, A, h)\n",
    "\n",
    "        return att_score, A, h, ins_labels, ins_logits_unnorm, ins_logits, slide_score_unnorm, Y_prob, Y_hat, Y_true, predict_slide_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CLAM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CLAM Model on the Given Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_optimize(img_features, slide_label, i_model, b_model, c_model, i_optimizer, b_optimizer, c_optimizer, \n",
    "                i_loss_func, b_loss_func, n_class, c1, c2, mutual_ex):\n",
    "    \n",
    "    with tf.GradientTape() as i_tape, tf.GradientTape() as b_tape, tf.GradientTape() as c_tape:\n",
    "        \n",
    "        att_score, A, h, ins_labels, ins_logits_unnorm, ins_logits, slide_score_unnorm, \\\n",
    "        Y_prob, Y_hat, Y_true, predict_slide_label = c_model.call(img_features, slide_label)\n",
    "\n",
    "        ins_labels, ins_logits_unnorm, ins_logits = i_model.call(slide_label, h, A)\n",
    "        ins_loss = list()\n",
    "        for j in range(len(ins_logits)):\n",
    "            i_loss = i_loss_func(tf.one_hot(ins_labels[j], 2), ins_logits[j])\n",
    "            ins_loss.append(i_loss)\n",
    "        if mutual_ex:\n",
    "            I_Loss = tf.math.add_n(ins_loss) / n_class\n",
    "        else:\n",
    "            I_Loss = tf.math.add_n(ins_loss)\n",
    "\n",
    "        slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true = b_model.call(slide_label, A, h)\n",
    "        \n",
    "        B_Loss = b_loss_func(Y_true, Y_prob)\n",
    "        \n",
    "        T_Loss = c1 * B_Loss + c2 * I_Loss\n",
    "\n",
    "    i_grad = i_tape.gradient(I_Loss, i_model.trainable_weights)\n",
    "    i_optimizer.apply_gradients(zip(i_grad, i_model.trainable_weights))\n",
    "\n",
    "    b_grad = b_tape.gradient(B_Loss, b_model.trainable_weights)\n",
    "    b_optimizer.apply_gradients(zip(b_grad, b_model.trainable_weights))\n",
    "\n",
    "    c_grad = c_tape.gradient(T_Loss, c_model.trainable_weights)\n",
    "    c_optimizer.apply_gradients(zip(c_grad, c_model.trainable_weights))\n",
    "    \n",
    "    return I_Loss, B_Loss, T_Loss, predict_slide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_optimize(batch_size, n_ins, n_samples, img_features, slide_label, i_model, b_model,\n",
    "               c_model, i_optimizer, b_optimizer, c_optimizer, i_loss_func, b_loss_func,\n",
    "               n_class, c1, c2, mutual_ex):\n",
    "    \n",
    "    step_size = 0\n",
    "    \n",
    "    Ins_Loss = list()\n",
    "    Bag_Loss = list()\n",
    "    Total_Loss = list()\n",
    "    \n",
    "    label_predict = list()\n",
    "    \n",
    "    for n_step in range(0, (n_samples // batch_size + 1)):\n",
    "        if step_size < (n_samples - batch_size):\n",
    "            with tf.GradientTape() as i_tape, tf.GradientTape() as b_tape, tf.GradientTape() as c_tape:\n",
    "                att_score, A, h, ins_labels, ins_logits_unnorm, ins_logits, slide_score_unnorm, \\\n",
    "                Y_prob, Y_hat, Y_true, predict_label = c_model.call(img_features=img_features[step_size:(step_size + batch_size)],\n",
    "                                                                    slide_label=slide_label)\n",
    "\n",
    "                ins_labels, ins_logits_unnorm, ins_logits = i_model.call(slide_label, h, A)\n",
    "                \n",
    "                ins_loss = list()\n",
    "                for j in range(len(ins_logits)):\n",
    "                    i_loss = i_loss_func(tf.one_hot(ins_labels[j], 2), ins_logits[j])\n",
    "                    ins_loss.append(i_loss)\n",
    "                if mutual_ex:\n",
    "                    Loss_I = tf.math.add_n(ins_loss) / n_class\n",
    "                else:\n",
    "                    Loss_I = tf.math.add_n(ins_loss)\n",
    "\n",
    "                slide_score_unnorm, Y_hat, Y_prob, predict_label, Y_true = b_model.call(slide_label, A, h)\n",
    "                \n",
    "                Loss_B = b_loss_func(Y_true, Y_prob)\n",
    "                \n",
    "                Loss_T = c1 * Loss_B + c2 * Loss_I\n",
    "\n",
    "            i_grad = i_tape.gradient(Loss_I, i_model.trainable_weights)\n",
    "            i_optimizer.apply_gradients(zip(i_grad, i_model.trainable_weights))\n",
    "\n",
    "            b_grad = b_tape.gradient(Loss_B, b_model.trainable_weights)\n",
    "            b_optimizer.apply_gradients(zip(b_grad, b_model.trainable_weights))\n",
    "\n",
    "            c_grad = c_tape.gradient(Loss_T, c_model.trainable_weights)\n",
    "            c_optimizer.apply_gradients(zip(c_grad, c_model.trainable_weights))\n",
    "    \n",
    "        else:\n",
    "            with tf.GradientTape() as i_tape, tf.GradientTape() as b_tape, tf.GradientTape() as c_tape:\n",
    "                att_score, A, h, ins_labels, ins_logits_unnorm, ins_logits, slide_score_unnorm, \\\n",
    "                Y_prob, Y_hat, Y_true, predict_label = c_model.call(img_features=img_features[(step_size - n_ins):],\n",
    "                                                                    slide_label=slide_label)\n",
    "\n",
    "                ins_labels, ins_logits_unnorm, ins_logits = i_model.call(slide_label, h, A)\n",
    "                \n",
    "                ins_loss = list()\n",
    "                for j in range(len(ins_logits)):\n",
    "                    i_loss = i_loss_func(tf.one_hot(ins_labels[j], 2), ins_logits[j])\n",
    "                    ins_loss.append(i_loss)\n",
    "                if mutual_ex:\n",
    "                    Loss_I = tf.math.add_n(ins_loss) / n_class\n",
    "                else:\n",
    "                    Loss_I = tf.math.add_n(ins_loss)\n",
    "\n",
    "                slide_score_unnorm, Y_hat, Y_prob, predict_label, Y_true = b_model.call(slide_label, A, h)\n",
    "                \n",
    "                Loss_B = b_loss_func(Y_true, Y_prob)\n",
    "                \n",
    "                Loss_T = c1 * Loss_B + c2 * Loss_I\n",
    "\n",
    "            i_grad = i_tape.gradient(Loss_I, i_model.trainable_weights)\n",
    "            i_optimizer.apply_gradients(zip(i_grad, i_model.trainable_weights))\n",
    "\n",
    "            b_grad = b_tape.gradient(Loss_B, b_model.trainable_weights)\n",
    "            b_optimizer.apply_gradients(zip(b_grad, b_model.trainable_weights))\n",
    "\n",
    "            c_grad = c_tape.gradient(Loss_T, c_model.trainable_weights)\n",
    "            c_optimizer.apply_gradients(zip(c_grad, c_model.trainable_weights))\n",
    "            \n",
    "        Ins_Loss.append(float(Loss_I))\n",
    "        Bag_Loss.append(float(Loss_B))\n",
    "        Total_Loss.append(float(Loss_T))\n",
    "        \n",
    "        label_predict.append(predict_label)\n",
    "        \n",
    "        step_size += batch_size\n",
    "    \n",
    "    I_Loss = statistics.mean(Ins_Loss)\n",
    "    B_Loss = statistics.mean(Bag_Loss)\n",
    "    T_Loss = statistics.mean(Total_Loss)\n",
    "    \n",
    "    predict_slide_label = most_frequent(label_predict)\n",
    "    \n",
    "    return I_Loss, B_Loss, T_Loss, predict_slide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(i_model, b_model, c_model, train_path, i_optimizer_func, b_optimizer_func,\n",
    "               c_optimizer_func, i_loss_func, b_loss_func, mutual_ex, n_class, c1, c2, \n",
    "               learn_rate, l2_decay, n_ins, batch_size, batch_op):\n",
    "    \n",
    "    loss_total = list()\n",
    "    loss_ins = list()\n",
    "    loss_bag = list()\n",
    "\n",
    "    i_optimizer = i_optimizer_func(learning_rate=learn_rate, weight_decay=l2_decay)\n",
    "    b_optimizer = b_optimizer_func(learning_rate=learn_rate, weight_decay=l2_decay)\n",
    "    c_optimizer = c_optimizer_func(learning_rate=learn_rate, weight_decay=l2_decay)\n",
    "\n",
    "    slide_true_label = list()\n",
    "    slide_predict_label = list()\n",
    "\n",
    "    train_sample_list = os.listdir(train_path)\n",
    "    train_sample_list = random.sample(train_sample_list, len(train_sample_list))\n",
    "    for i in train_sample_list:\n",
    "        print('=', end=\"\")\n",
    "        single_train_data = train_path + i\n",
    "        img_features, slide_label = get_data_from_tf(single_train_data)\n",
    "        # shuffle the order of img features list in order to reduce the side effects of randomly drop potential \n",
    "        # number of patches' feature vectors during training when enable batch training option\n",
    "        img_features = random.sample(img_features, len(img_features))\n",
    "        \n",
    "        if batch_op:\n",
    "            I_Loss, B_Loss, T_Loss, predict_slide_label = b_optimize(batch_size=batch_size, n_ins=n_ins, n_samples=len(img_features), \n",
    "                                                                     img_features=img_features, slide_label=slide_label, \n",
    "                                                                     i_model=i_model, b_model=b_model, c_model=c_model, \n",
    "                                                                     i_optimizer=i_optimizer, b_optimizer=b_optimizer, \n",
    "                                                                     c_optimizer=c_optimizer, i_loss_func=i_loss_func, \n",
    "                                                                     b_loss_func = b_loss_func, n_class=n_class, c1=c1, \n",
    "                                                                     c2=c2, mutual_ex=mutual_ex)\n",
    "        else:\n",
    "            I_Loss, B_Loss, T_Loss, predict_slide_label = nb_optimize(img_features=img_features, slide_label=slide_label,\n",
    "                                                                      i_model=i_model, b_model=b_model, c_model=c_model, \n",
    "                                                                      i_optimizer=i_optimizer, b_optimizer=b_optimizer, \n",
    "                                                                      c_optimizer=c_optimizer, i_loss_func=i_loss_func, \n",
    "                                                                      b_loss_func=b_loss_func, n_class=n_class, c1=c1, c2=c2, \n",
    "                                                                      mutual_ex=mutual_ex)\n",
    "\n",
    "        loss_total.append(float(T_Loss))\n",
    "        loss_ins.append(float(I_Loss))\n",
    "        loss_bag.append(float(B_Loss))\n",
    "\n",
    "        slide_true_label.append(slide_label)\n",
    "        slide_predict_label.append(predict_slide_label)\n",
    "\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(slide_true_label, slide_predict_label).ravel()\n",
    "    train_tn = int(tn)\n",
    "    train_fp = int(fp)\n",
    "    train_fn = int(fn)\n",
    "    train_tp = int(tp)\n",
    "\n",
    "    train_sensitivity = round(train_tp / (train_tp + train_fn), 2)\n",
    "    train_specificity = round(train_tn / (train_tn + train_fp), 2)\n",
    "    train_acc = round((train_tp + train_tn) / (train_tn + train_fp + train_fn + train_tp), 2)\n",
    "\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(slide_true_label, slide_predict_label, pos_label=1)\n",
    "    train_auc = round(sklearn.metrics.auc(fpr, tpr), 2)\n",
    "\n",
    "    train_loss = statistics.mean(loss_total)\n",
    "    train_ins_loss = statistics.mean(loss_ins)\n",
    "    train_bag_loss = statistics.mean(loss_bag)\n",
    "\n",
    "    return train_loss, train_ins_loss, train_bag_loss, train_tn, train_fp, train_fn, train_tp, train_sensitivity, \\\n",
    "           train_specificity, train_acc, train_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_val(img_features, slide_label, i_model, b_model, c_model, \n",
    "           i_loss_func, b_loss_func, n_class, c1, c2, mutual_ex):\n",
    "    \n",
    "    att_score, A, h, ins_labels, ins_logits_unnorm, ins_logits, slide_score_unnorm, \\\n",
    "    Y_prob, Y_hat, Y_true, predict_slide_label = c_model.call(img_features, slide_label)\n",
    " \n",
    "    ins_labels, ins_logits_unnorm, ins_logits = i_model.call(slide_label, h, A)\n",
    "    \n",
    "    ins_loss = list()\n",
    "    for j in range(len(ins_logits)):\n",
    "        i_loss = i_loss_func(tf.one_hot(ins_labels[j], 2), ins_logits[j])\n",
    "        ins_loss.append(i_loss)\n",
    "    if mutual_ex:\n",
    "        I_Loss = tf.math.add_n(ins_loss) / n_class\n",
    "    else:\n",
    "        I_Loss = tf.math.add_n(ins_loss)\n",
    "\n",
    "    slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true = b_model.call(slide_label, A, h)\n",
    "\n",
    "    B_Loss = b_loss_func(Y_true, Y_prob)\n",
    "    \n",
    "    T_Loss = c1 * B_Loss + c2 * I_Loss\n",
    "    \n",
    "    return I_Loss, B_Loss, T_Loss, predict_slide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_val(batch_size, n_ins, n_samples, img_features, slide_label, i_model, b_model,\n",
    "          c_model, i_loss_func, b_loss_func, n_class, c1, c2, mutual_ex):\n",
    "    \n",
    "    step_size = 0\n",
    "    \n",
    "    Ins_Loss = list()\n",
    "    Bag_Loss = list()\n",
    "    Total_Loss = list()\n",
    "    \n",
    "    label_predict = list()\n",
    "    \n",
    "    for n_step in range(0, (n_samples // batch_size + 1)):\n",
    "        if step_size < (n_samples - batch_size):\n",
    "            att_score, A, h, ins_labels, ins_logits_unnorm, ins_logits, slide_score_unnorm, \\\n",
    "            Y_prob, Y_hat, Y_true, predict_label = c_model.call(img_features=img_features[step_size:(step_size + batch_size)],\n",
    "                                                                slide_label=slide_label)\n",
    "\n",
    "            ins_labels, ins_logits_unnorm, ins_logits = i_model.call(slide_label, h, A)\n",
    "            \n",
    "            ins_loss = list()\n",
    "            for j in range(len(ins_logits)):\n",
    "                i_loss = i_loss_func(tf.one_hot(ins_labels[j], 2), ins_logits[j])\n",
    "                ins_loss.append(i_loss)\n",
    "            if mutual_ex:\n",
    "                Loss_I = tf.math.add_n(ins_loss) / n_class\n",
    "            else:\n",
    "                Loss_I = tf.math.add_n(ins_loss)\n",
    "\n",
    "            slide_score_unnorm, Y_hat, Y_prob, predict_label, Y_true = b_model.call(slide_label, A, h)\n",
    "            \n",
    "            Loss_B = b_loss_func(Y_true, Y_prob)\n",
    "            Loss_T = c1 * Loss_B + c2 * Loss_I\n",
    "            \n",
    "        else:\n",
    "            att_score, A, h, ins_labels, ins_logits_unnorm, ins_logits, slide_score_unnorm, \\\n",
    "            Y_prob, Y_hat, Y_true, predict_label = c_model.call(img_features=img_features[(step_size - n_ins):],\n",
    "                                                                slide_label=slide_label)\n",
    "\n",
    "            ins_labels, ins_logits_unnorm, ins_logits = i_model.call(slide_label, h, A)\n",
    "            \n",
    "            ins_loss = list()\n",
    "            for j in range(len(ins_logits)):\n",
    "                i_loss = i_loss_func(tf.one_hot(ins_labels[j], 2), ins_logits[j])\n",
    "                ins_loss.append(i_loss)\n",
    "            if mutual_ex:\n",
    "                Loss_I = tf.math.add_n(ins_loss) / n_class\n",
    "            else:\n",
    "                Loss_I = tf.math.add_n(ins_loss)\n",
    "\n",
    "            slide_score_unnorm, Y_hat, Y_prob, predict_label, Y_true = b_model.call(slide_label, A, h)\n",
    "            \n",
    "            Loss_B = b_loss_func(Y_true, Y_prob)\n",
    "            \n",
    "            Loss_T = c1 * Loss_B + c2 * Loss_I\n",
    "        \n",
    "        Ins_Loss.append(float(Loss_I))\n",
    "        Bag_Loss.append(float(Loss_B))\n",
    "        Total_Loss.append(float(Loss_T))\n",
    "\n",
    "        label_predict.append(predict_label)\n",
    "        \n",
    "        step_size += batch_size\n",
    "    \n",
    "    I_Loss = statistics.mean(Ins_Loss)\n",
    "    B_Loss = statistics.mean(Bag_Loss)\n",
    "    T_Loss = statistics.mean(Total_Loss)\n",
    "    \n",
    "    predict_slide_label = most_frequent(label_predict)\n",
    "\n",
    "    return I_Loss, B_Loss, T_Loss, predict_slide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(i_model, b_model, c_model, val_path, i_loss_func, b_loss_func, mutual_ex, \n",
    "             n_class, c1, c2, n_ins, batch_size, batch_op):\n",
    "    loss_t = list()\n",
    "    loss_i = list()\n",
    "    loss_b = list()\n",
    "\n",
    "    slide_true_label = list()\n",
    "    slide_predict_label = list()\n",
    "\n",
    "    val_sample_list = os.listdir(val_path)\n",
    "    val_sample_list = random.sample(val_sample_list, len(val_sample_list))\n",
    "    for i in val_sample_list:\n",
    "        print('=', end=\"\")\n",
    "        single_val_data = val_path + i\n",
    "        img_features, slide_label = get_data_from_tf(single_val_data)\n",
    "        \n",
    "        img_features = random.sample(img_features, len(img_features)) # follow the training loop, see details there\n",
    "                                    \n",
    "        if batch_op:\n",
    "            I_Loss, B_Loss, T_Loss, predict_slide_label = b_val(batch_size=batch_size, n_ins=n_ins, n_samples=len(img_features),\n",
    "                                                                img_features=img_features, slide_label=slide_label, \n",
    "                                                                i_model=i_model, b_model=b_model, c_model=c_model, \n",
    "                                                                i_loss_func=i_loss_func, b_loss_func=b_loss_func, \n",
    "                                                                n_class=n_class, c1=c1, c2=c2, mutual_ex=mutual_ex)\n",
    "        else:    \n",
    "            I_Loss, B_Loss, T_Loss, predict_slide_label = nb_val(img_features=img_features, slide_label=slide_label, \n",
    "                                                                 i_model=i_model, b_model=b_model, c_model=c_model, \n",
    "                                                                 i_loss_func=i_loss_func, b_loss_func=b_loss_func, \n",
    "                                                                 n_class=n_class, c1=c1, c2=c2, mutual_ex=mutual_ex)\n",
    "\n",
    "        loss_t.append(float(T_Loss))\n",
    "        loss_i.append(float(I_Loss))\n",
    "        loss_b.append(float(B_Loss))\n",
    "\n",
    "        slide_true_label.append(slide_label)\n",
    "        slide_predict_label.append(predict_slide_label)\n",
    "\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(slide_true_label, slide_predict_label).ravel()\n",
    "    val_tn = int(tn)\n",
    "    val_fp = int(fp)\n",
    "    val_fn = int(fn)\n",
    "    val_tp = int(tp)\n",
    "\n",
    "    val_sensitivity = round(val_tp / (val_tp + val_fn), 2)\n",
    "    val_specificity = round(val_tn / (val_tn + val_fp), 2)\n",
    "    val_acc = round((val_tp + val_tn) / (val_tn + val_fp + val_fn + val_tp), 2)\n",
    "\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(slide_true_label, slide_predict_label, pos_label=1)\n",
    "    val_auc = round(sklearn.metrics.auc(fpr, tpr), 2)\n",
    "\n",
    "    val_loss = statistics.mean(loss_t)\n",
    "    val_ins_loss = statistics.mean(loss_i)\n",
    "    val_bag_loss = statistics.mean(loss_b)\n",
    "\n",
    "    return val_loss, val_ins_loss, val_bag_loss, val_tn, val_fp, val_fn, val_tp, val_sensitivity, val_specificity, \\\n",
    "           val_acc, val_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Optimized CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i_model, b_model, c_model, test_path, result_path, result_file_name):\n",
    "    start_time = time.time()\n",
    "\n",
    "    slide_true_label = list()\n",
    "    slide_predict_label = list()\n",
    "    sample_names = list()\n",
    "\n",
    "    for i in os.listdir(test_path):\n",
    "        print('>', end=\"\")\n",
    "        single_test_data = test_path + i\n",
    "        img_features, slide_label = get_data_from_tf(single_test_data)\n",
    "\n",
    "        att_score, A, h, ins_labels, ins_logits_unnorm, ins_logits, slide_score_unnorm, \\\n",
    "        Y_prob, Y_hat, Y_true, predict_slide_label = c_model.call(img_features, slide_label)\n",
    "    \n",
    "        ins_labels, ins_logits_unnorm, ins_logits = i_model.call(slide_label, h, A)\n",
    "        \n",
    "        slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true = b_model.call(slide_label, A, h)\n",
    "    \n",
    "        slide_true_label.append(slide_label)\n",
    "        slide_predict_label.append(predict_slide_label)\n",
    "        sample_names.append(i)\n",
    "\n",
    "        test_results = pd.DataFrame(list(zip(sample_names, slide_true_label, slide_predict_label)),\n",
    "                                    columns=['Sample Names', 'Slide True Label', 'Slide Predict Label'])\n",
    "        test_results.to_csv(os.path.join(result_path, result_file_name), sep='\\t', index=False)\n",
    "\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(slide_true_label, slide_predict_label).ravel()\n",
    "    test_tn = int(tn)\n",
    "    test_fp = int(fp)\n",
    "    test_fn = int(fn)\n",
    "    test_tp = int(tp)\n",
    "\n",
    "    test_sensitivity = round(test_tp / (test_tp + test_fn), 2)\n",
    "    test_specificity = round(test_tn / (test_tn + test_fp), 2)\n",
    "    test_acc = round((test_tp + test_tn) / (test_tn + test_fp + test_fn + test_tp), 2)\n",
    "\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(slide_true_label, slide_predict_label, pos_label=1)\n",
    "    test_auc = round(sklearn.metrics.auc(fpr, tpr), 2)\n",
    "\n",
    "    test_run_time = time.time() - start_time\n",
    "\n",
    "    template = '\\n Test Accuracy: {}, Test Sensitivity: {}, Test Specificity: {}, Test Running Time: {}'\n",
    "    print(template.format(f\"{float(test_acc):.4%}\",\n",
    "                          f\"{float(test_sensitivity):.4%}\",\n",
    "                          f\"{float(test_specificity):.4%}\",\n",
    "                          \"--- %s mins ---\" % int(test_run_time / 60)))\n",
    "\n",
    "    return test_tn, test_fp, test_fn, test_tp, test_sensitivity, test_specificity, test_acc, test_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & Restoring CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save(i_model, b_model, c_model, i_model_dir, b_model_dir, c_model_dir, n_class, m_bag_op, m_clam_op, g_att_op):\n",
    "    for i in range(n_class):\n",
    "        i_model.ins_classifier()[i].save(os.path.join(i_model_dir, 'M_Ins', 'Class_' + str(i)))\n",
    "        \n",
    "    if m_bag_op:\n",
    "        for j in range(n_class):\n",
    "            b_model.bag_classifier()[j].save(os.path.join(b_model_dir, 'M_Bag', 'Class_' + str(j)))\n",
    "    else:\n",
    "        b_model.bag_classifier().save(os.path.join(b_model_dir, 'S_Bag'))\n",
    "    \n",
    "    clam_model_names = ['_Att', '_Ins', '_Bag']\n",
    "                                         \n",
    "    if m_clam_op:                                 \n",
    "        if g_att_op:\n",
    "            att_nets = c_model.clam_model()[0]\n",
    "            for m in range(len(att_nets)):\n",
    "                att_nets[m].save(os.path.join(c_model_dir, 'G' + clam_model_names[0], 'Model_' + str(m + 1)))\n",
    "        else:\n",
    "            att_nets = c_model.clam_model()[0]\n",
    "            for m in range(len(att_nets)):\n",
    "                att_nets[m].save(os.path.join(c_model_dir, 'NG' + clam_model_names[0], 'Model_' + str(m + 1)))                             \n",
    "                                         \n",
    "        for n in range(n_class):\n",
    "            ins_nets = c_model.clam_model()[1]\n",
    "            bag_nets = c_model.clam_model()[2]\n",
    "            \n",
    "            ins_nets[n].save(os.path.join(c_model_dir, 'M' + clam_model_names[1], 'Class_' + str(n)))\n",
    "            bag_nets[n].save(os.path.join(c_model_dir, 'M' + clam_model_names[2], 'Class_' + str(n)))\n",
    "    else:\n",
    "        if g_att_op:\n",
    "            att_nets = c_model.clam_model()[0]\n",
    "            for m in range(len(att_nets)):\n",
    "                att_nets[m].save(os.path.join(c_model_dir, 'G' + clam_model_names[0], 'Model_' + str(m + 1)))\n",
    "        else:\n",
    "            att_nets = c_model.clam_model()[0]\n",
    "            for m in range(len(att_nets)):\n",
    "                att_nets[m].save(os.path.join(c_model_dir, 'NG' + clam_model_names[0], 'Model_' + str(m + 1)))\n",
    "                                         \n",
    "        for n in range(n_class):\n",
    "            ins_nets = c_model.clam_model()[1]\n",
    "            ins_nets[n].save(os.path.join(c_model_dir, 'M' + clam_model_names[1], 'Class_' + str(n)))\n",
    "        \n",
    "        c_model.clam_model()[2].save(os.path.join(c_model_dir, 'S' + clam_model_names[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_model(i_model_dir, b_model_dir, c_model_dir, n_class, m_bag_op, m_clam_op, g_att_op):\n",
    "    i_trained_model = list()\n",
    "    for i in range(n_class):\n",
    "        m_ins_names = os.listdir(os.path.join(i_model_dir, 'M_Ins'))\n",
    "        m_ins_names.sort()\n",
    "        m_ins_name = m_ins_names[i]\n",
    "        m_ins_model = tf.keras.models.load_model(os.path.join(i_model_dir, 'M_Ins', m_ins_name))\n",
    "        i_trained_model.append(m_ins_model)\n",
    "        \n",
    "    if m_bag_op:\n",
    "        b_trained_model = list()\n",
    "        for j in range(n_class):\n",
    "            m_bag_names = os.listdir(os.path.join(b_model_dir, 'M_Bag'))\n",
    "            m_bag_names.sort()\n",
    "            m_bag_name = m_bag_names[j]                                     \n",
    "            m_bag_model = tf.keras.models.load_model(os.path.join(b_model_dir, 'M_Bag', m_bag_name))\n",
    "            b_trained_model.append(m_bag_model)\n",
    "    else:\n",
    "        s_bag_name = os.listdir(b_model_dir)[0]\n",
    "        b_trained_model = tf.keras.models.load_model(os.path.join(b_model_dir, s_bag_name))\n",
    "    \n",
    "    clam_model_names = ['_Att', '_Ins', '_Bag']\n",
    " \n",
    "    trained_att_net = list()\n",
    "    trained_ins_classifier = list()\n",
    "    trained_bag_classifier = list()\n",
    "    \n",
    "    c_trained_model = list()\n",
    "    \n",
    "    if m_clam_op:\n",
    "        if g_att_op:\n",
    "            att_nets_dir = os.path.join(c_model_dir, 'G' + clam_model_names[0])\n",
    "            for k in range(len(os.listdir(att_nets_dir))):\n",
    "                att_net = tf.keras.models.load_model(os.path.join(att_nets_dir, 'Model_' + str(k+1)))\n",
    "                trained_att_net.append(att_net)\n",
    "        else:\n",
    "            att_nets_dir = os.path.join(c_model_dir, 'NG' + clam_model_names[0])\n",
    "            for k in range(len(os.listdir(att_nets_dir))):\n",
    "                att_net = tf.keras.models.load_model(os.path.join(att_nets_dir, 'Model_' + str(k+1)))\n",
    "                trained_att_net.append(att_net)\n",
    "        \n",
    "        ins_nets_dir = os.path.join(c_model_dir, 'M' + clam_model_names[1])\n",
    "        bag_nets_dir = os.path.join(c_model_dir, 'M' + clam_model_names[2])\n",
    "        \n",
    "        for m in range(n_class):\n",
    "            ins_net = tf.keras.models.load_model(os.path.join(ins_nets_dir, 'Class_' + str(m)))\n",
    "            bag_net = tf.keras.models.load_model(os.path.join(bag_nets_dir, 'Class_' + str(m)))\n",
    "            \n",
    "            trained_ins_classifier.append(ins_net)\n",
    "            trained_bag_classifier.append(bag_net)\n",
    "        \n",
    "        c_trained_model = [trained_att_net, trained_ins_classifier, trained_bag_classifier]\n",
    "    else:\n",
    "        if g_att_op:\n",
    "            att_nets_dir = os.path.join(c_model_dir, 'G' + clam_model_names[0])\n",
    "            for k in range(len(os.listdir(att_nets_dir))):\n",
    "                att_net = tf.keras.models.load_model(os.path.join(att_nets_dir, 'Model_' + str(k + 1)))\n",
    "                trained_att_net.append(att_net)\n",
    "        else:\n",
    "            att_nets_dir = os.path.join(c_model_dir, 'NG' + clam_model_names[0])\n",
    "            for k in range(len(os.listdir(att_nets_dir))):\n",
    "                att_net = tf.keras.models.load_model(os.path.join(att_nets_dir, 'Model_' + str(k + 1)))\n",
    "                trained_att_net.append(att_net)\n",
    "        \n",
    "        ins_nets_dir = os.path.join(c_model_dir, 'M' + clam_model_names[1])\n",
    "        \n",
    "        for m in range(n_class):\n",
    "            ins_net = tf.keras.models.load_model(os.path.join(ins_nets_dir, 'Class_' + str(m)))\n",
    "            trained_ins_classifier.append(ins_net)\n",
    "            \n",
    "        bag_nets_dir = os.path.join(c_model_dir, 'S' + clam_model_names[2])\n",
    "        trained_bag_classifier.append(tf.keras.models.load_model(bag_nets_dir))\n",
    "        \n",
    "        c_trained_model = [trained_att_net, trained_ins_classifier, trained_bag_classifier[0]]\n",
    "    \n",
    "    return i_trained_model, b_trained_model, c_trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & Restoring CLAM Model Training Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing CLAM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_att = NG_Att_Net(dim_features=1024, dim_compress_features=512, n_hidden_units=256, n_classes=2,\n",
    "                 dropout=False, dropout_rate=.25)\n",
    "\n",
    "g_att = G_Att_Net(dim_features=1024, dim_compress_features=512, n_hidden_units=256, n_classes=2,\n",
    "                 dropout=False, dropout_rate=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = Ins(dim_compress_features=512, n_class=2, n_ins=8, mut_ex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_bag = S_Bag(dim_compress_features=512, n_class=2)\n",
    "\n",
    "m_bag = M_Bag(dim_compress_features=512, n_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_clam = S_CLAM(att_gate=True, net_size='big', n_ins=8, n_class=2, mut_ex=False,\n",
    "            dropout=True, drop_rate=.55, mil_ins=True, att_only=False)\n",
    "\n",
    "m_clam = M_CLAM(att_gate=True, net_size='big', n_ins=8, n_class=2, mut_ex=False,\n",
    "            dropout=True, drop_rate=.55, mil_ins=True, att_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Required Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nis_bach = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/BACH/No_Image_Standardization/train/'\n",
    "val_nis_bach = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/BACH/No_Image_Standardization/val/'\n",
    "test_nis_bach = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/BACH/No_Image_Standardization/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_is_bach = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/BACH/Image_Standardization/train/'\n",
    "val_is_bach = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/BACH/Image_Standardization/val/'\n",
    "test_is_bach = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/BACH/Image_Standardization/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nis_tcga = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/TCGA/No_Image_Standardization/train/'\n",
    "val_nis_tcga = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/TCGA/No_Image_Standardization/val/'\n",
    "test_nis_tcga = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/TCGA/No_Image_Standardization/test/'\n",
    "extra_nis_tcga = 'research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/TCGA/No_Image_Standardization/extra/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_is_tcga = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/TCGA/Image_Standardization/train/'\n",
    "val_is_tcga = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/TCGA/Image_Standardization/val/'\n",
    "test_is_tcga = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/TCGA/Image_Standardization/test/'\n",
    "extra_is_tcga = 'research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/TCGA/Image_Standardization/extra/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "clam_result_dir = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_trained_model_dir = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/Saved_Model/Ins_Classifier'\n",
    "b_trained_model_dir = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/Saved_Model/Bag_Classifier'\n",
    "c_trained_model_dir = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/Quincy/Data/CLAM/Saved_Model/CLAM_Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/' \\\n",
    "'Quincy/Data/CLAM/log/' + current_time + '/train'\n",
    "val_log_dir = '/research/bsi/projects/PI/tertiary/Hart_Steven_m087494/s211408.DigitalPathology/' \\\n",
    "'Quincy/Data/CLAM/log/' + current_time + '/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validating CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(train_log, val_log, train_path, val_path, i_model, b_model,\n",
    "               c_model, i_optimizer_func, b_optimizer_func, c_optimizer_func, \n",
    "               i_loss_func, b_loss_func, mutual_ex, n_class, c1, c2, learn_rate, \n",
    "               l2_decay, n_ins, batch_size, batch_op, epochs):\n",
    "    \n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log)\n",
    "    val_summary_writer = tf.summary.create_file_writer(val_log)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training Step\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_ins_loss, train_bag_loss, train_tn, train_fp, train_fn, train_tp, \\\n",
    "        train_sensitivity, train_specificity, train_acc, train_auc = train_step(\n",
    "            i_model=i_model, b_model=b_model, c_model=c_model, train_path=train_path,\n",
    "            i_optimizer_func=i_optimizer_func, b_optimizer_func=b_optimizer_func, \n",
    "            c_optimizer_func=c_optimizer_func, i_loss_func=i_loss_func, \n",
    "            b_loss_func=b_loss_func, mutual_ex=mutual_ex, n_class=n_class, \n",
    "            c1=c1, c2=c2, learn_rate=learn_rate, l2_decay=l2_decay, \n",
    "            n_ins=n_ins, batch_size=batch_size, batch_op=batch_op)\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('Total Loss', float(train_loss), step=epoch)\n",
    "            tf.summary.scalar('Instance Loss', float(train_ins_loss), step=epoch)\n",
    "            tf.summary.scalar('Bag Loss', float(train_bag_loss), step=epoch)\n",
    "            tf.summary.scalar('Accuracy', float(train_acc), step=epoch)\n",
    "            tf.summary.scalar('AUC', float(train_auc), step=epoch)\n",
    "            tf.summary.scalar('Sensitivity', float(train_sensitivity), step=epoch)\n",
    "            tf.summary.scalar('Specificity', float(train_specificity), step=epoch)\n",
    "            tf.summary.histogram('True Positive', int(train_tp), step=epoch)\n",
    "            tf.summary.histogram('False Positive', int(train_fp), step=epoch)\n",
    "            tf.summary.histogram('True Negative', int(train_tn), step=epoch)\n",
    "            tf.summary.histogram('False Negative', int(train_fn), step=epoch)\n",
    "\n",
    "        # Validation Step\n",
    "        val_loss, val_ins_loss, val_bag_loss, val_tn, val_fp, val_fn, val_tp, \\\n",
    "        val_sensitivity, val_specificity, val_acc, val_auc = val_step(\n",
    "            i_model=i_model, b_model=b_model, c_model=c_model, val_path=val_path,\n",
    "            i_loss_func=i_loss_func, b_loss_func=b_loss_func, mutual_ex=mutual_ex, \n",
    "            n_class=n_class, c1=c1, c2=c2, n_ins=n_ins, batch_size=batch_size, batch_op=batch_op)\n",
    "\n",
    "        with val_summary_writer.as_default():\n",
    "            tf.summary.scalar('Total Loss', float(val_loss), step=epoch)\n",
    "            tf.summary.scalar('Instance Loss', float(val_ins_loss), step=epoch)\n",
    "            tf.summary.scalar('Bag Loss', float(val_bag_loss), step=epoch)\n",
    "            tf.summary.scalar('Accuracy', float(val_acc), step=epoch)\n",
    "            tf.summary.scalar('AUC', float(val_auc), step=epoch)\n",
    "            tf.summary.scalar('Sensitivity', float(val_sensitivity), step=epoch)\n",
    "            tf.summary.scalar('Specificity', float(val_specificity), step=epoch)\n",
    "            tf.summary.histogram('True Positive', int(val_tp), step=epoch)\n",
    "            tf.summary.histogram('False Positive', int(val_fp), step=epoch)\n",
    "            tf.summary.histogram('True Negative', int(val_tn), step=epoch)\n",
    "            tf.summary.histogram('False Negative', int(val_fn), step=epoch)\n",
    "\n",
    "        epoch_run_time = time.time() - start_time\n",
    "        template = '\\n Epoch {},  Train Loss: {}, Train Accuracy: {}, Val Loss: {}, Val Accuracy: {}, Epoch Running ' \\\n",
    "                   'Time: {} '\n",
    "        print(template.format(epoch + 1,\n",
    "                              f\"{float(train_loss):.8}\",\n",
    "                              f\"{float(train_acc):.4%}\",\n",
    "                              f\"{float(val_loss):.8}\",\n",
    "                              f\"{float(val_acc):.4%}\",\n",
    "                              \"--- %s mins ---\" % int(epoch_run_time / 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function to Optimizing and Testing CLAM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1 - Test the Optimized CLAM Model by Saving the Trained CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clam_optimize(train_log, val_log, train_path, val_path, i_model, b_model,\n",
    "                  c_model, i_optimizer_func, b_optimizer_func, c_optimizer_func, \n",
    "                  i_loss_func, b_loss_func, mutual_ex, n_class, c1, c2, learn_rate, \n",
    "                  l2_decay, n_ins, batch_size, batch_op, i_model_dir, b_model_dir, \n",
    "                  c_model_dir, m_bag_op, m_clam_op, g_att_op, epochs):\n",
    "    \n",
    "    train_val(train_log=train_log, val_log=val_log, train_path=train_path,\n",
    "               val_path=val_path, i_model=i_model, b_model=b_model, c_model=c_model,\n",
    "               i_optimizer_func=i_optimizer_func, b_optimizer_func=b_optimizer_func,\n",
    "               c_optimizer_func=c_optimizer_func, i_loss_func=i_loss_func,\n",
    "               b_loss_func=b_loss_func, mutual_ex=mutual_ex, n_class=n_class, \n",
    "               c1=c1, c2=c2, learn_rate=learn_rate, l2_decay=l2_decay, \n",
    "               n_ins=n_ins, batch_size=batch_size, batch_op=batch_op, epochs=epochs)\n",
    "    \n",
    "    model_save(i_model=i_model, b_model=b_model, c_model=c_model, \n",
    "               i_model_dir=i_model_dir, b_model_dir=b_model_dir, \n",
    "               c_model_dir=c_model_dir, n_class=n_class, m_bag_op=m_bag_op, \n",
    "               m_clam_op=m_clam_op, g_att_op=g_att_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clam_test(test_path, result_path, result_file_name, \n",
    "              i_model_dir, b_model_dir, c_model_dir, \n",
    "              n_class, m_bag_op, m_clam_op, g_att_op):\n",
    "    \n",
    "    i_trained_model, b_trained_model, c_trained_model = restore_model(i_model_dir=i_model_dir, \n",
    "                                                                      b_model_dir=b_model_dir, \n",
    "                                                                      c_model_dir=c_model_dir, \n",
    "                                                                      n_class=n_class, m_bag_op=m_bag_op, \n",
    "                                                                      m_clam_op=m_clam_op, g_att_op=g_att_op)\n",
    "    \n",
    "    test_tn, test_fp, test_fn, test_tp, test_sensitivity, test_specificity, \\\n",
    "    test_acc, test_auc = test(i_model=ins, b_model=b_trained_model, \n",
    "                              c_model=s_clam, test_path=test_path, \n",
    "                              result_path=result_path, result_file_name=result_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2 - Test the Optimized CLAM Model Withought Saving the Trained CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_test(train_log, val_log, train_path, val_path, test_path, result_path, result_file_name,\n",
    "              i_model, b_model, c_model, i_optimizer_func, b_optimizer_func,\n",
    "              c_optimizer_func, i_loss_func, b_loss_func, mutual_ex,\n",
    "              n_class, c1, c2, learn_rate, l2_decay, n_ins, batch_size, batch_op, epochs):\n",
    "    \n",
    "    train_val(train_log=train_log, val_log=val_log, train_path=train_path,\n",
    "               val_path=val_path, i_model=i_model, b_model=b_model, c_model=c_model,\n",
    "               i_optimizer_func=i_optimizer_func, b_optimizer_func=b_optimizer_func,\n",
    "               c_optimizer_func=c_optimizer_func, i_loss_func=i_loss_func,\n",
    "               b_loss_func=b_loss_func, mutual_ex=mutual_ex, n_class=n_class, \n",
    "               c1=c1, c2=c2, learn_rate=learn_rate, l2_decay=l2_decay, \n",
    "               n_ins=n_ins, batch_size=batch_size, batch_op=batch_op, epochs=epochs)\n",
    "\n",
    "    test_tn, test_fp, test_fn, test_tp, test_sensitivity, test_specificity, \\\n",
    "    test_acc, test_auc = test(i_model=i_model, b_model=b_model, c_model=c_model, \n",
    "                              test_path=test_path, result_path=result_path,\n",
    "                              result_file_name=result_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training, Validating & Testing CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_shut_up(no_warn_op=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================================================================================================================================================================================================================================================================\n",
      " Epoch 1,  Train Loss: 8.1606448, Train Accuracy: 54.0000%, Val Loss: 9.9709798, Val Accuracy: 50.0000%, Epoch Running Time: --- 1 mins --- \n"
     ]
    }
   ],
   "source": [
    "clam_optimize(train_log=train_log_dir, val_log=val_log_dir, \n",
    "              train_path=train_is_bach, val_path=val_is_bach, \n",
    "              i_model=ins, b_model=s_bag, c_model=s_clam, \n",
    "              i_optimizer_func=tfa.optimizers.AdamW, \n",
    "              b_optimizer_func=tfa.optimizers.AdamW, \n",
    "              c_optimizer_func=tfa.optimizers.AdamW, \n",
    "              i_loss_func=tf.keras.losses.binary_crossentropy, \n",
    "              b_loss_func=tf.keras.losses.binary_crossentropy, \n",
    "              mutual_ex=False, n_class=2, c1=0.7, c2=0.3, \n",
    "              learn_rate=2e-04, l2_decay=1e-05, n_ins=8, \n",
    "              batch_size=2000, batch_op=False, \n",
    "              i_model_dir=i_trained_model_dir, \n",
    "              b_model_dir=b_trained_model_dir, \n",
    "              c_model_dir=c_trained_model_dir, \n",
    "              m_bag_op=False, m_clam_op=False, g_att_op=True, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_trained_model, b_trained_model, c_trained_model = restore_model(i_model_dir=i_trained_model_dir, \n",
    "                                                                  b_model_dir=b_trained_model_dir, \n",
    "                                                                  c_model_dir=c_trained_model_dir, \n",
    "                                                                  n_class=2, m_bag_op=False, \n",
    "                                                                  m_clam_op=False, g_att_op=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "clam_test(test_path=test_is_bach, result_path=clam_result_dir, \n",
    "          result_file_name='test_bach_save_model.tsv', \n",
    "          i_model_dir=i_trained_model_dir, \n",
    "          b_model_dir=b_trained_model_dir, \n",
    "          c_model_dir=c_trained_model_dir, \n",
    "          n_class=2, m_bag_op=False, \n",
    "          m_clam_op=False, g_att_op=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================================================================================================================================================================================================================================================================\n",
      " Epoch 1,  Train Loss: 7.6590552, Train Accuracy: 50.0000%, Val Loss: 7.6483054, Val Accuracy: 50.0000%, Epoch Running Time: --- 1 mins --- \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      " Test Accuracy: 50.0000%, Test Sensitivity: 100.0000%, Test Specificity: 0.0000%, Test Running Time: --- 0 mins ---\n"
     ]
    }
   ],
   "source": [
    "optimize_test(train_log=train_log_dir, val_log=val_log_dir, train_path=train_is_bach,\n",
    "          val_path=val_is_bach, test_path=test_is_bach, result_path=clam_result_dir, \n",
    "          result_file_name='test_bach.tsv', i_model=ins, b_model=s_bag, c_model=s_clam,\n",
    "          i_optimizer_func=tfa.optimizers.AdamW, b_optimizer_func=tfa.optimizers.AdamW,\n",
    "          c_optimizer_func=tfa.optimizers.AdamW, i_loss_func=tf.keras.losses.binary_crossentropy,\n",
    "          b_loss_func=tf.keras.losses.binary_crossentropy, mutual_ex=True, n_class=2,\n",
    "          c1=0.7, c2=0.3, learn_rate=2e-04, l2_decay=1e-05, n_ins=8, batch_size=1000, batch_op=False, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

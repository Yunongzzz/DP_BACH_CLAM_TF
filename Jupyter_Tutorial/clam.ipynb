{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Image Feature and Slide Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_tf(tf_path):\n",
    "    feature = {'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "               'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "               'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "               'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "               'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "               'image_name': tf.io.FixedLenFeature([], tf.string),\n",
    "               'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "               'image_feature': tf.io.FixedLenFeature([], tf.string)}\n",
    "\n",
    "    tfrecord_dataset = tf.data.TFRecordDataset(tf_path)\n",
    "\n",
    "    def _parse_image_function(key):\n",
    "        return tf.io.parse_single_example(key, feature)\n",
    "\n",
    "    CLAM_dataset = tfrecord_dataset.map(_parse_image_function)\n",
    "\n",
    "    Image_Features = list()\n",
    "\n",
    "    for tfrecord_value in CLAM_dataset:\n",
    "        img_features = tf.io.parse_tensor(tfrecord_value['image_feature'], 'float32')\n",
    "        slide_labels = tfrecord_value['label']\n",
    "        slide_label = int(slide_labels)\n",
    "        Image_Features.append(img_features)\n",
    "\n",
    "    return Image_Features, slide_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Dataset and Split into Training, Validation, and Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord = 'path/to/tfrecord'\n",
    "clam_dir = 'path/to/user/selected/training-validation-testing/path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_shuffle(dataset, path, percent=[0.8,0.1,0.1]):\n",
    "    \"\"\"\n",
    "    Input Arg:\n",
    "        dataset -> path where all tfrecord data stored\n",
    "        path -> path where you want to save training, testing, and validation data folder\n",
    "    \"\"\"\n",
    "    \n",
    "    # return training, validation, and testing path name\n",
    "    train = path + '/train'\n",
    "    valid = path + '/valid'\n",
    "    test = path + '/test'\n",
    "    \n",
    "    # create training, validation, and testing directory only if it is not existed\n",
    "    if os.path.exists(train) == False:\n",
    "        os.mkdir(os.path.join(clam_dir,'train'))\n",
    "    if os.path.exists(valid) == False:\n",
    "        os.mkdir(os.path.join(clam_dir,'valid'))\n",
    "    if os.path.exists(test) == False:\n",
    "        os.mkdir(os.path.join(clam_dir,'test'))\n",
    "    \n",
    "    total_num_data = len(os.listdir(dataset))\n",
    " \n",
    "    # only shuffle the data when train, validation, and test directory are all empty\n",
    "    if len(os.listdir(train)) == 0 & len(os.listdir(valid)) == 0 & len(os.listdir(test)) == 0:\n",
    "        train_names = random.sample(os.listdir(dataset), int(total_num_data*percent[0]))  \n",
    "        for i in train_names:\n",
    "            train_srcpath = os.path.join(dataset, i)\n",
    "            shutil.copy(train_srcpath, train)\n",
    "        \n",
    "        valid_names = random.sample(list(set(os.listdir(dataset)) - set(os.listdir(train))), int(total_num_data*percent[1]))\n",
    "        for j in valid_names:\n",
    "            valid_srcpath = os.path.join(dataset, j)\n",
    "            shutil.copy(valid_srcpath, valid)\n",
    "        \n",
    "        test_names = list(set(os.listdir(dataset)) - set(os.listdir(train)) - set(os.listdir(valid)))\n",
    "        for k in test_names:\n",
    "            test_srcpath = os.path.join(dataset, k)\n",
    "            shutil.copy(test_srcpath, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shuffle(tfrecord, clam_dir, percent=[0.7,0.15,0.15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CLAM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import None-Gated Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None-Gated Attention Network Class - assign the same weights of each attention head/layer\n",
    "class NG_Att_Net(tf.keras.Model):\n",
    "    def __init__(self, dim_features=1024, dim_compress_features=512, n_hidden_units=256, n_classes=2,\n",
    "                 dropout=False, dropout_rate=.25):\n",
    "        super(NG_Att_Net, self).__init__()\n",
    "        self.dim_features = dim_features\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.compression_model = tf.keras.models.Sequential()\n",
    "        self.model = tf.keras.models.Sequential()\n",
    "\n",
    "        self.fc_compress_layer = tf.keras.layers.Dense(units=dim_compress_features, activation='relu',\n",
    "                                                       input_shape=(dim_features,), kernel_initializer='glorot_normal',\n",
    "                                                       bias_initializer='zeros', name='Fully_Connected_Layer')\n",
    "\n",
    "        self.compression_model.add(self.fc_compress_layer)\n",
    "        self.model.add(self.fc_compress_layer)\n",
    "\n",
    "        self.att_layer1 = tf.keras.layers.Dense(units=n_hidden_units, activation='tanh',\n",
    "                                                input_shape=(dim_compress_features,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer1')\n",
    "\n",
    "        self.att_layer2 = tf.keras.layers.Dense(units=n_classes, activation='linear', input_shape=(n_hidden_units,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer2')\n",
    "\n",
    "        self.model.add(self.att_layer1)\n",
    "\n",
    "        if dropout:\n",
    "            self.model.add(tf.keras.layers.Dropout(dropout_rate, name='Dropout_Layer'))\n",
    "\n",
    "        self.model.add(self.att_layer2)\n",
    "\n",
    "    def att_compress_model_no_gate(self):\n",
    "        return self.compression_model\n",
    "\n",
    "    def att_model_no_gate(self):\n",
    "        return self.model\n",
    "\n",
    "    def compress(self, x):\n",
    "        h = list()\n",
    "        for i in x:\n",
    "            c_imf = self.compression_model(i)\n",
    "            h.append(c_imf)\n",
    "        return h\n",
    "\n",
    "    def forward(self, x):\n",
    "        A = list()\n",
    "        for i in x:\n",
    "            a = self.model(i)\n",
    "            A.append(a)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Gated Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gated Attention Network Class - scaling the weights of each attention head/layer -> weights of each attention layer\n",
    "# would be different\n",
    "class G_Att_Net(tf.keras.Model):\n",
    "    def __init__(self, dim_features=1024, dim_compress_features=512, n_hidden_units=256, n_classes=2,\n",
    "                 dropout=False, dropout_rate=.25):\n",
    "        super(G_Att_Net, self).__init__()\n",
    "        self.dim_features = dim_features\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.compression_model = tf.keras.models.Sequential()\n",
    "        self.model1 = tf.keras.models.Sequential()\n",
    "        self.model2 = tf.keras.models.Sequential()\n",
    "        self.model = tf.keras.models.Sequential()\n",
    "\n",
    "        # GlorotNormal <==> Xavier for weights initialization\n",
    "        self.fc_compress_layer = tf.keras.layers.Dense(units=dim_compress_features, activation='relu',\n",
    "                                                       input_shape=(dim_features,), kernel_initializer='glorot_normal',\n",
    "                                                       bias_initializer='zeros', name='Fully_Connected_Layer')\n",
    "\n",
    "        self.compression_model.add(self.fc_compress_layer)\n",
    "        self.model1.add(self.fc_compress_layer)\n",
    "        self.model2.add(self.fc_compress_layer)\n",
    "\n",
    "        self.att_layer1 = tf.keras.layers.Dense(units=n_hidden_units, activation='tanh', input_shape=(dim_features,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer1')\n",
    "\n",
    "        self.att_layer2 = tf.keras.layers.Dense(units=n_hidden_units, activation='sigmoid', input_shape=(dim_features,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer2')\n",
    "\n",
    "        self.att_layer3 = tf.keras.layers.Dense(units=n_classes, activation='linear', input_shape=(n_hidden_units,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer3')\n",
    "\n",
    "        self.model1.add(self.att_layer1)\n",
    "        self.model2.add(self.att_layer2)\n",
    "\n",
    "        if dropout:\n",
    "            self.model1.add(tf.keras.layers.Dropout(dropout_rate, name='Dropout_Layer'))\n",
    "            self.model2.add(tf.keras.layers.Dropout(dropout_rate, name='Dropout_Layer'))\n",
    "\n",
    "        self.model.add(self.att_layer3)\n",
    "\n",
    "    def att_compress_model_gate(self):\n",
    "        return self.compression_model\n",
    "\n",
    "    def att_model_gate(self):\n",
    "        gated_att_net_list = [self.model1, self.model2, self.model]\n",
    "        return gated_att_net_list\n",
    "\n",
    "    def compress(self, x):\n",
    "        h = list()\n",
    "        for i in x:\n",
    "            c_imf = self.compression_model(i)\n",
    "            h.append(c_imf)\n",
    "        return h\n",
    "\n",
    "    def forward(self, x):\n",
    "        A = list()\n",
    "        for i in x:\n",
    "            layer1_output = self.model1(i)  # output from the first dense layer\n",
    "            layer2_output = self.model2(i)  # output from the second dense layer\n",
    "            a = tf.math.multiply(layer1_output, layer2_output)  # cross product of the outputs from 1st and 2nd layer\n",
    "            a = self.model(a)  # pass the output of the product of the outputs from 1st 2 layers to the last layer\n",
    "            A.append(a)\n",
    "\n",
    "        return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLAM Class - Attention Network (Gated/None-Gated) + Instance-Level Clustering\n",
    "class CLAM(tf.keras.Model):\n",
    "    def __init__(self, att_net_gate=False, net_siz_arg='small', n_instance_sample=8, n_classes=2, subtype_prob=False,\n",
    "                 dropout=False, dropout_rate=.25, mil_loss_func=tf.keras.losses.CategoricalCrossentropy()):\n",
    "        super(CLAM, self).__init__()\n",
    "        self.att_net_gate = att_net_gate\n",
    "        self.net_size_arg = net_siz_arg\n",
    "        self.n_instance_sample = n_instance_sample\n",
    "        self.n_classes = n_classes\n",
    "        self.subtype_prob = subtype_prob\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mil_loss_func = mil_loss_func\n",
    "\n",
    "        self.size_dictionary = {\n",
    "            'small': [1024, 512, 256],\n",
    "            'big': [1024, 512, 384]\n",
    "        }\n",
    "        size = self.size_dictionary[net_siz_arg]\n",
    "\n",
    "        if att_net_gate:\n",
    "            self.att_net = G_Att_Net(dim_features=size[0], dim_compress_features=size[1], n_hidden_units=size[2],\n",
    "                                     n_classes=n_classes, dropout=dropout, dropout_rate=dropout_rate)\n",
    "        else:\n",
    "            self.att_net = NG_Att_Net(dim_features=size[0], dim_compress_features=size[1], n_hidden_units=size[2],\n",
    "                                      n_classes=n_classes, dropout=dropout, dropout_rate=dropout_rate)\n",
    "\n",
    "        # Multi-Instance Learning - Adding 2 classifier models, one for bag-level, one for instance-level\n",
    "        # Bag-level classifier model\n",
    "        self.bag_classifiers = list()  # list of keras sequential model w/ single linear dense layer for each class\n",
    "        for i in range(n_classes):\n",
    "            self.bag_classifier = tf.keras.models.Sequential(\n",
    "                tf.keras.layers.Dense(units=1, activation='linear', input_shape=(size[1],))  # W_[c,m] shape be (1,512)\n",
    "            )  # independent sequential model w/ single linear dense layer to do slide-level prediction for each class\n",
    "            self.bag_classifiers.append(self.bag_classifier)\n",
    "\n",
    "        # Instance-level classifier model\n",
    "        # for each of n classes, take transpose of compressed img feature for kth patch (h_k) with shape (512,1) in,\n",
    "        # and return the cluster assignment score predicted for kth patch (P_[m,k]) with shape (2,1)\n",
    "        self.instance_classifiers = list()\n",
    "        for i in range(n_classes):\n",
    "            self.instance_classifier = tf.keras.models.Sequential(\n",
    "                tf.keras.layers.Dense(units=self.n_classes, activation='linear', input_shape=(size[1],))\n",
    "            )   # W_[inst,m] shape (2,512)\n",
    "            self.instance_classifiers.append(self.instance_classifier)\n",
    "            \n",
    "    # Generate patch-level pseudo labels with staticmethod [default values -> 1 for positive, 0 for negative]\n",
    "    # Generate positive patch-level pseudo labels\n",
    "    @staticmethod\n",
    "    def generate_pos_labels(n_pos_sample):\n",
    "        return tf.fill(dims=[n_pos_sample, ], value=1.0)\n",
    "\n",
    "    # Generate negative patch-level pseudo labels\n",
    "    @staticmethod\n",
    "    def generate_neg_labels(n_neg_sample):\n",
    "        return tf.fill(dims=[n_neg_sample, ], value=0.0)\n",
    "\n",
    "    # Self-defined function equivalent to torch.index_select() with staticmethod\n",
    "    # Usage -> get top k pos/neg instances based on the generated indexes by sorting their attention scores\n",
    "    @staticmethod\n",
    "    def tf_index_select(input, dim, index):\n",
    "        \"\"\"\n",
    "        input_(tensor): input tensor\n",
    "        dim(int): dimension\n",
    "        index (LongTensor)  the 1-D tensor containing the indices to index\n",
    "        \"\"\"\n",
    "        shape = input.get_shape().as_list()\n",
    "        if dim == -1:\n",
    "            dim = len(shape) - 1\n",
    "        shape[dim] = 1\n",
    "\n",
    "        tmp = []\n",
    "        for idx in index:\n",
    "            begin = [0] * len(shape)\n",
    "            begin[dim] = idx\n",
    "            tmp.append(tf.slice(input, begin, shape))\n",
    "        res = tf.concat(tmp, axis=dim)\n",
    "\n",
    "        return res\n",
    "\n",
    "    # Apply Multi-Instance Learning to perform in-class and out-class instance-level clustering\n",
    "    # In-class attention branch based instance-level clustering\n",
    "    def instance_clustering_in_class(self, A, h, classifier):\n",
    "        pos_pseudo_labels = self.generate_pos_labels(self.n_instance_sample)\n",
    "        neg_pseudo_labels = self.generate_neg_labels(self.n_instance_sample)\n",
    "        pseudo_labels = tf.concat(values=[pos_pseudo_labels, neg_pseudo_labels], axis=0)\n",
    "        A = tf.reshape(tf.convert_to_tensor(A), (1, len(A)*self.n_classes)) \n",
    "            \n",
    "        top_pos_ids = tf.math.top_k(A, self.n_instance_sample)[1][-1]  \n",
    "        pos_index = list()\n",
    "        for i in top_pos_ids:\n",
    "            if i%2 == 0:\n",
    "                pos_index.append(i//2)\n",
    "            else:\n",
    "                pos_index.append((i+1)//2)\n",
    "        pos_index = tf.convert_to_tensor(pos_index)\n",
    "        top_pos = list()\n",
    "        for i in pos_index:\n",
    "            top_pos.append(h[i])\n",
    " \n",
    "        top_neg_ids = tf.math.top_k(-A, self.n_instance_sample)[1][-1]\n",
    "        neg_index = list()\n",
    "        for i in top_neg_ids:\n",
    "            if i%2 == 0:\n",
    "                neg_index.append(i//2)\n",
    "            else:\n",
    "                neg_index.append((i+1)//2)\n",
    "        neg_index = tf.convert_to_tensor(neg_index)\n",
    "        top_neg = list()\n",
    "        for i in neg_index:\n",
    "            top_neg.append(h[i])\n",
    "\n",
    "        instance_samples = tf.concat(values=[top_pos, top_neg], axis=0)\n",
    "\n",
    "        logits = list()\n",
    "        instance_loss = list()\n",
    "        \n",
    "        for i in range(self.n_instance_sample):\n",
    "            logit = tf.reshape(classifier(instance_samples[i]), (2,1))\n",
    "            ins_loss = self.mil_loss_func(pseudo_labels[i], logit)\n",
    "            logits.append(logit)\n",
    "            instance_loss.append(ins_loss)\n",
    "\n",
    "        instance_predict = tf.sort(logits, direction='ASCENDING')\n",
    "        instance_predict = tf.reshape(tf.convert_to_tensor(instance_predict), (1, len(instance_predict)*self.n_classes))\n",
    "        pos_predict = instance_predict[0][:self.n_instance_sample]\n",
    "        neg_predict = instance_predict[0][self.n_instance_sample:]\n",
    "        accurate_pos_predict = (pos_predict == tf.constant(pos_pseudo_labels)).numpy().tolist().count(True)\n",
    "        accurate_neg_predict = (neg_predict == tf.constant(neg_pseudo_labels)).numpy().tolist().count(True)\n",
    "\n",
    "        pos_acc = accurate_pos_predict / self.n_instance_sample\n",
    "        neg_acc = accurate_neg_predict / self.n_instance_sample\n",
    "\n",
    "        return instance_loss, pos_acc, neg_acc\n",
    "\n",
    "    # Out-class attention branch based instance-level clustering [Optional Functionality]\n",
    "    def instance_level_clustering_out_class(self, A, h, classifier):\n",
    "        # get compressed 512-dimensional instance-level feature vectors for following use, denoted by h\n",
    "        A = tf.reshape(tf.convert_to_tensor(A), (1, len(A)*self.n_classes))\n",
    "        top_pos_ids = tf.math.top_k(A, self.n_instance_sample)[1][-1]\n",
    "        pos_index = list()\n",
    "        for i in top_pos_ids:\n",
    "            if i%2 == 0:\n",
    "                pos_index.append(i//2)\n",
    "            else:\n",
    "                pos_index.append((i+1)//2)\n",
    "        pos_index = tf.convert_to_tensor(pos_index)\n",
    "        top_pos = list()\n",
    "        for i in pos_index:\n",
    "            top_pos.append(h[i])\n",
    "\n",
    "        # mutually-exclusive -> top k instances w/ highest attention scores ==> false pos = neg\n",
    "        pos_pseudo_labels = self.generate_neg_labels(self.n_instance_sample)\n",
    "        logits = list()\n",
    "        instance_loss = list()\n",
    "        \n",
    "        for i in range(self.n_instance_sample):\n",
    "            logit = tf.reshape(classifier(top_pos[i]), (2,1))\n",
    "            ins_loss = self.mil_loss_func(pos_pseudo_labels[i], logit)\n",
    "            logits.append(logit)\n",
    "            instance_loss.append(ins_loss)\n",
    "\n",
    "        pos_predict = tf.sort(logits, direction='ASCENDING')\n",
    "        pos_predict = tf.reshape(tf.convert_to_tensor(pos_predict), (1, len(pos_predict)*self.n_classes))\n",
    "        pos_predict = pos_predict[0][:self.n_instance_sample]\n",
    "        accurate_pos_predict = (pos_predict == tf.constant(pos_pseudo_labels)).numpy().tolist().count(True)\n",
    "        pos_acc = accurate_pos_predict / self.n_instance_sample\n",
    "\n",
    "        # top k instances w/ lowest attention scores -> false neg != pos ==> excluded\n",
    "        neg_acc = -1  # never pick top k neg instances in out-the-class instance-level clustering, set this be -1\n",
    "\n",
    "        return instance_loss, pos_acc, neg_acc\n",
    "\n",
    "    def forward(self, img_features, slide_label, mil_op=False, slide_predict_op=False, att_only_op=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_features -> original 1024-dimensional instance-level feature vectors\n",
    "            labels -> mutable entire label set, could be 0 or 1 for binary classification\n",
    "            mil_op -> whether or not perform the instance-level clustering, default be False\n",
    "            slide_predict_op ->\n",
    "            att_only_op -> if only return the attention scores, default be False\n",
    "        \"\"\"\n",
    "\n",
    "        # get the compressed 512-dim feature vectors for following use\n",
    "        h = self.att_net.compress(img_features)\n",
    "\n",
    "        A = self.att_net.forward(img_features)\n",
    "        att_net_out = A  # output from attention network\n",
    "        A = tf.math.softmax(A)   # attention scores computed by Eqa#1 in CLAM paper\n",
    "\n",
    "        if att_only_op:\n",
    "            CLAM_outcomes = {\n",
    "                'Attention_Scores': att_net_out\n",
    "            }\n",
    "            return CLAM_outcomes  # return attention scores of the kth patch for the mth class (i.e. a_[k,m])\n",
    "\n",
    "        if mil_op:\n",
    "            instance_loss_total = 0.0\n",
    "            pos_acc_total = 0.0\n",
    "            neg_acc_total = 0.0\n",
    "\n",
    "            for i in range(len(self.instance_classifiers)):\n",
    "\n",
    "                classifier = self.instance_classifiers[i]\n",
    "                if i == slide_label:\n",
    "                    instance_loss, pos_acc, neg_acc = self.instance_clustering_in_class(A, h, classifier)\n",
    "            \n",
    "                    pos_acc_total += pos_acc\n",
    "                    neg_acc_total += neg_acc\n",
    "                else:\n",
    "                    if self.subtype_prob:  # classes are mutually-exclusive assumption holds\n",
    "                        instance_loss, pos_acc, neg_acc = self.instance_level_clustering_out_class(A, h, classifier)\n",
    "                        pos_acc += pos_acc\n",
    "                    else:    # classes are mutually-exclusive assumption not holds\n",
    "                        continue\n",
    "                instance_loss_total = sum(instance_loss)\n",
    "\n",
    "            if self.subtype_prob:\n",
    "                pos_acc_total /= len(self.instance_classifiers)\n",
    "                instance_loss_total /= len(self.instance_classifiers)\n",
    "\n",
    "            CLAM_outcomes = {\n",
    "                'Instance_Loss': instance_loss_total,\n",
    "                'Prediction_Accuracy_Positive': pos_acc_total\n",
    "            }\n",
    "        else:\n",
    "            CLAM_outcomes = {}\n",
    "\n",
    "        # compute the slide-level representation aggregated per the attention score distribution for the mth class\n",
    "        SAR = list()\n",
    "        for i in range(len(img_features)):\n",
    "            sar = tf.linalg.matmul(tf.transpose(A[i]), h[i])  # return h_[slide,m], shape be (2,512)\n",
    "            SAR.append(sar)\n",
    "        slide_agg_rep = tf.add_n(SAR)\n",
    "        \n",
    "        if slide_predict_op:\n",
    "            CLAM_outcomes.update({\n",
    "                'Slide_Level_Representation': slide_agg_rep\n",
    "            })\n",
    "\n",
    "        # unnormalized slide-level score (s_[slide,m]) with uninitialized entries, shape be (1,num_of_classes)\n",
    "        slide_score_unnorm = tf.Variable(np.empty((1, self.n_classes)), dtype=tf.float32)\n",
    "\n",
    "        # return s_[slide,m] (slide-level prediction scores)\n",
    "        for j in range(self.n_classes):\n",
    "            ssu = self.bag_classifiers[j](tf.reshape(slide_agg_rep[j], (1,512)))[0,0]\n",
    "            tf.compat.v1.assign(slide_score_unnorm[0,j], ssu)\n",
    "\n",
    "        Y_hat = tf.math.top_k(slide_score_unnorm, 1)[1][-1]\n",
    "        Y_prob = tf.compat.v1.math.softmax(slide_score_unnorm)\n",
    "\n",
    "        return att_net_out, slide_score_unnorm, Y_hat, Y_prob, CLAM_outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CLAM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'path/to/training/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CLAM Model on the Given Training Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clam_train(train_path, learning_rate=2e-04, beta_1=0.9, beta_2=0.999, epsilon=1e-07):\n",
    "    \"\"\"\n",
    "    Input Arg:\n",
    "        train_path -> path of the training data\n",
    "    \"\"\"\n",
    "    \n",
    "    clam = CLAM(att_net_gate=False, net_siz_arg='small', n_instance_sample=8, n_classes=2, subtype_prob=False, dropout=False, \n",
    "            dropout_rate=.25, mil_loss_func=tf.keras.losses.CategoricalCrossentropy())\n",
    "    \n",
    "    clam_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-04, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "    for i in os.listdir(train_path):\n",
    "        single_train_data = train_path + i\n",
    "        img_features = get_data_from_tf(single_train_data)\n",
    "        slide_label = 0\n",
    "            \n",
    "        with tf.GradientTape() as clam_tape:\n",
    "            A = clam.forward(img_features, slide_label, mil_op=False, slide_predict_op=False, att_only_op=True)\n",
    "            clam_output = clam.forward(img_features, slide_label, mil_op=True, slide_predict_op=False, att_only_op=False)\n",
    "            mil_loss = clam.forward(img_features, slide_label, mil_op=True, slide_predict_op=False, att_only_op=False)[4]['Instance_Loss']\n",
    "                \n",
    "        grad_clam = clam_tape.gradient(mil_loss, clam.trainable_variables)\n",
    "        clam_optimizer.apply_gradients(zip(grad_clam, clam.trainable_variables))\n",
    "    \n",
    "    return {'Instance Loss': mil_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Training Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Training Loss in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Trained CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}